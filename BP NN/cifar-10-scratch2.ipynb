{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pickle\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def load_cifar10_batch(file_path):\n",
    "#     \"\"\"\n",
    "#     读取单个CIFAR-10数据批次\n",
    "#     \"\"\"\n",
    "#     with open(file_path, 'rb') as file:\n",
    "#         # 先读取全部数据\n",
    "#         data = np.fromfile(file, dtype=np.uint8)\n",
    "    \n",
    "#     # 每个样本的第一个字节是标签，接下来的3072个字节是图像数据\n",
    "#     labels = data[::3073]  # 每3073个字节的第一个字节是标签\n",
    "#     images = []\n",
    "\n",
    "#     for i in range(10000):  # 每个批次有10000个图像\n",
    "#         start = i * 3073 + 1  # 每个图像数据块的起始位置（跳过标签）\n",
    "#         end = start + 3072    # 每个图像数据块的结束位置\n",
    "#         image = data[start:end].reshape(3, 32, 32).transpose(1, 2, 0).astype(\"float\")\n",
    "#         images.append(image)\n",
    "    \n",
    "#     images = np.array(images)\n",
    "#     return images, labels\n",
    "\n",
    "\n",
    "\n",
    "# def one_hot_encode(labels, num_classes=10):\n",
    "#     \"\"\"\n",
    "#     将标签进行独热编码\n",
    "#     \"\"\"\n",
    "#     one_hot_labels = np.eye(num_classes)[labels]\n",
    "#     return one_hot_labels\n",
    "\n",
    "\n",
    "# def load_data():\n",
    "#     \"\"\"\n",
    "#     加载所有训练和测试数据\n",
    "#     \"\"\"\n",
    "#     train_images = []\n",
    "#     train_labels = []\n",
    "\n",
    "#     # 文件路径列表\n",
    "#     files = [\n",
    "#         \"data/cifar10/train/data_batch_1.bin\",\n",
    "#         \"data/cifar10/train/data_batch_2.bin\",\n",
    "#         \"data/cifar10/train/data_batch_3.bin\",\n",
    "#         \"data/cifar10/train/data_batch_4.bin\",\n",
    "#         \"data/cifar10/train/data_batch_5.bin\"\n",
    "#     ]\n",
    "\n",
    "#     # 加载训练数据\n",
    "#     for file in files:\n",
    "#         images, labels = load_cifar10_batch(file)\n",
    "#         train_images.append(images)\n",
    "#         train_labels.append(labels)\n",
    "    \n",
    "#     train_images = np.concatenate(train_images)\n",
    "#     train_labels = np.concatenate(train_labels)\n",
    "\n",
    "#     # 加载测试数据\n",
    "#     test_images, test_labels = load_cifar10_batch(\"data/cifar10/test/test_batch.bin\")\n",
    "\n",
    "#     return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "\n",
    "# # 加载数据\n",
    "# x_train, y_train, x_test, y_test = load_data()\n",
    "\n",
    "# # 打印数据形状\n",
    "# print(\"Training data shape:\", x_train.shape)\n",
    "# print(\"Training labels shape:\", y_train.shape)\n",
    "# print(\"Test data shape:\", x_test.shape)\n",
    "# print(\"Test labels shape:\", y_test.shape)\n",
    "\n",
    "# # 数据标准化\n",
    "# x_train = x_train / 255.0\n",
    "# x_test = x_test / 255.0\n",
    "\n",
    "# # One-hot 编码标签\n",
    "# y_train_encoded = one_hot_encode(y_train, num_classes=10)\n",
    "# y_test_encoded = one_hot_encode(y_test, num_classes=10)\n",
    "\n",
    "# # 打印处理后的数据形状\n",
    "# print(\"x_train shape:\", x_train.shape)\n",
    "# print(\"y_train_encoded shape:\", y_train_encoded.shape)\n",
    "# print(\"x_test shape:\", x_test.shape)\n",
    "# print(\"y_test_encoded shape:\", y_test_encoded.shape)\n",
    "\n",
    "# # 可视化前50个训练集图像\n",
    "# fig, axes = plt.subplots(5, 10, figsize=(15, 7))\n",
    "# fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "# for i, ax in enumerate(axes.flat):\n",
    "#     ax.imshow(x_train[i])\n",
    "#     ax.axis('off')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(out):\n",
    "    '''\n",
    "    Description: Sigmoid Activation\n",
    "    Params: out = a list/matrix to perform the activation on\n",
    "    Outputs: Sigmoid activated list/matrix\n",
    "    '''\n",
    "    return 1.0 / (1.0 + np.exp(-out))\n",
    "\n",
    "\n",
    "def delta_sigmoid(out):\n",
    "    '''\n",
    "    Description: Derivative of Sigmoid Activation\n",
    "    Params: out = a list/matrix to perform the activation on\n",
    "    Outputs: Delta(Sigmoid) activated list/matrix\n",
    "    '''\n",
    "    return sigmoid(out) * (1 - sigmoid(out))\n",
    "\n",
    "def SigmoidCrossEntropyLoss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid cross-entropy loss.\n",
    "    \"\"\"\n",
    "    m = y_true.shape[1]  # number of examples\n",
    "    cost = -(1 / m) * np.sum(np.nan_to_num(y_true * np.log(y_pred + 1e-9) + (1 - y_true) * np.log(1 - y_pred + 1e-9)))\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records in Training Dataset 40500\n",
      "Records in Validation Dataset 4500\n",
      "Training data shape: (40500, 32, 32, 3)\n",
      "Training labels shape: (40500, 10)\n",
      "Validation data shape: (4500, 32, 32, 3)\n",
      "Validation labels shape: (4500, 10)\n",
      "Test data shape: (10000, 32, 32, 3)\n",
      "Test labels shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_cifar10_batch(file_path):\n",
    "    \"\"\"\n",
    "    读取单个CIFAR-10数据批次\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = np.fromfile(file, dtype=np.uint8)\n",
    "    \n",
    "    labels = data[::3073]  # 每3073个字节的第一个字节是标签\n",
    "    images = []\n",
    "\n",
    "    for i in range(10000):  # 每个批次有10000个图像\n",
    "        start = i * 3073 + 1  # 每个图像数据块的起始位置（跳过标签）\n",
    "        end = start + 3072    # 每个图像数据块的结束位置\n",
    "        image = data[start:end].reshape(3, 32, 32).transpose(1, 2, 0).astype(\"float\")\n",
    "        images.append(image)\n",
    "    \n",
    "    images = np.array(images)\n",
    "    return images, labels\n",
    "\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    \"\"\"\n",
    "    将标签进行独热编码\n",
    "    \"\"\"\n",
    "    one_hot_labels = np.eye(num_classes)[labels]\n",
    "    return one_hot_labels\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    加载所有训练和测试数据\n",
    "    \"\"\"\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "\n",
    "    files = [\n",
    "        \"data/cifar10/train/data_batch_1.bin\",\n",
    "        \"data/cifar10/train/data_batch_2.bin\",\n",
    "        \"data/cifar10/train/data_batch_3.bin\",\n",
    "        \"data/cifar10/train/data_batch_4.bin\",\n",
    "        \"data/cifar10/train/data_batch_5.bin\"\n",
    "    ]\n",
    "\n",
    "    for file in files:\n",
    "        images, labels = load_cifar10_batch(file)\n",
    "        train_images.append(images)\n",
    "        train_labels.append(labels)\n",
    "    \n",
    "    train_images = np.concatenate(train_images)\n",
    "    train_labels = np.concatenate(train_labels)\n",
    "\n",
    "    test_images, test_labels = load_cifar10_batch(\"data/cifar10/test/test_batch.bin\")\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_data()\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "y_train_encoded = one_hot_encode(y_train, num_classes=10)\n",
    "y_test_encoded = one_hot_encode(y_test, num_classes=10)\n",
    "\n",
    "x_train, x_valid = x_train[5000:], x_train[:5000]\n",
    "y_train_encoded, y_valid_encoded = y_train_encoded[5000:], y_train_encoded[:5000]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Training labels shape:\", y_train_encoded.shape)\n",
    "print(\"Validation data shape:\", x_valid.shape)\n",
    "print(\"Validation labels shape:\", y_valid_encoded.shape)\n",
    "print(\"Test data shape:\", x_test.shape)\n",
    "print(\"Test labels shape:\", y_test_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def delta_sigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def SigmoidCrossEntropyLoss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid cross-entropy loss.\n",
    "    \"\"\"\n",
    "    m = y_true.shape[0]  # number of examples\n",
    "    if m == 0:\n",
    "        return 0\n",
    "    cost = -(1 / m) * np.sum(np.nan_to_num(y_true * np.log(y_pred + 1e-9) + (1 - y_true) * np.log(1 - y_pred + 1e-9)))\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    '''\n",
    "    Description: Class to define the Deep Neural Network\n",
    "    '''\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def forwardPropagation(self, x):\n",
    "        activation = x\n",
    "        activations = [x]  # list to store activations for every layer\n",
    "        outs = []  # list to store out vectors for every layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            out = np.dot(w, activation) + b\n",
    "            outs.append(out)\n",
    "            activation = sigmoid(out)\n",
    "            activations.append(activation)\n",
    "        return outs, activations\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        for batch_idx in range(0, X.shape[0], batch_size):\n",
    "            batch = zip(X[batch_idx:batch_idx + batch_size],\n",
    "                        y[batch_idx:batch_idx + batch_size])\n",
    "            yield batch\n",
    "\n",
    "    def train(self, X, y, x_val, y_val, batch_size=100, learning_rate=0.01, epochs=100):\n",
    "        n_batches = X.shape[0] // batch_size\n",
    "        for j in range(epochs):\n",
    "            batch_iter = self.get_batch(X, y, batch_size)\n",
    "            for i in range(n_batches):\n",
    "                batch = next(batch_iter)\n",
    "                batch_X = np.array([x for x, _ in batch])\n",
    "                batch_y = np.array([_ for _, y in batch])\n",
    "\n",
    "                del_b = [np.zeros(b.shape) for b in self.biases]\n",
    "                del_w = [np.zeros(w.shape) for w in self.weights]\n",
    "                loss, delta_del_b, delta_del_w = self.backPropagation(batch_X, batch_y)\n",
    "                del_b = [db + ddb for db, ddb in zip(del_b, delta_del_b)]\n",
    "                del_w = [dw + ddw for dw, ddw in zip(del_w, delta_del_w)]\n",
    "                \n",
    "                self.weights = [w - (learning_rate / batch_size) * delw for w, delw in zip(self.weights, del_w)]\n",
    "                self.biases = [b - (learning_rate / batch_size) * delb for b, delb in zip(self.biases, del_b)]\n",
    "            \n",
    "            acc_val = self.eval(x_val, y_val)\n",
    "            acc_train = self.eval(X, y)\n",
    "            \n",
    "            print(\"\\nEpoch %d complete\\t training accuracy: %f\\n\" % (j, acc_train))\n",
    "            print(\"\\nEpoch %d complete\\t validation accuracy: %f\\n\" % (j, acc_val))\n",
    "            print(\"\\nEpoch %d complete\\t training Loss: %f\\n\" % (j, loss))\n",
    "\n",
    "    def backPropagation(self, x, y):\n",
    "        del_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        del_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        outs, activations = self.forwardPropagation(x.T)\n",
    "\n",
    "        loss = SigmoidCrossEntropyLoss(activations[-1], y.T)\n",
    "\n",
    "        delta_cost = activations[-1] - y.T\n",
    "\n",
    "        delta = delta_cost\n",
    "        del_b[-1] = delta\n",
    "        del_w[-1] = np.dot(delta, activations[-2].T)\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            out = outs[-l]\n",
    "            delta_activation = delta_sigmoid(out)\n",
    "            delta = np.dot(self.weights[-l + 1].T, delta) * delta_activation\n",
    "            del_b[-l] = delta\n",
    "            del_w[-l] = np.dot(delta, activations[-l - 1].T)\n",
    "        return (loss, del_b, del_w)\n",
    "\n",
    "    def eval(self, X, y):\n",
    "        count = 0\n",
    "        for x, _y in zip(X, y):\n",
    "            outs, activations = self.forwardPropagation(x.T)\n",
    "            if np.argmax(activations[-1], axis=0) == np.argmax(_y, axis=0):\n",
    "                count += 1\n",
    "        return float(count) / X.shape[0] * 100\n",
    "\n",
    "    def predict(self, X):\n",
    "        labels = unpickle(\"cifar-10-batches-py/batches.meta\")[b'label_names']\n",
    "        preds = np.array([])\n",
    "        for x in X:\n",
    "            outs, activations = self.forwardPropagation(x.T)\n",
    "            preds = np.append(preds, np.argmax(activations[-1], axis=0))\n",
    "        preds = np.array([labels[int(p)] for p in preds])\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,100) (0,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m x_test_flat \u001b[38;5;241m=\u001b[39m x_test\u001b[38;5;241m.\u001b[39mreshape(x_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m dnn \u001b[38;5;241m=\u001b[39m DNN([input_size, hidden_size, output_size])\n\u001b[1;32m---> 10\u001b[0m dnn\u001b[38;5;241m.\u001b[39mtrain(x_train_flat, y_train_encoded, x_valid_flat, y_valid_encoded, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 40\u001b[0m, in \u001b[0;36mDNN.train\u001b[1;34m(self, X, y, x_val, y_val, batch_size, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     38\u001b[0m del_b \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mzeros(b\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases]\n\u001b[0;32m     39\u001b[0m del_w \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mzeros(w\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights]\n\u001b[1;32m---> 40\u001b[0m loss, delta_del_b, delta_del_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackPropagation(batch_X, batch_y)\n\u001b[0;32m     41\u001b[0m del_b \u001b[38;5;241m=\u001b[39m [db \u001b[38;5;241m+\u001b[39m ddb \u001b[38;5;28;01mfor\u001b[39;00m db, ddb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(del_b, delta_del_b)]\n\u001b[0;32m     42\u001b[0m del_w \u001b[38;5;241m=\u001b[39m [dw \u001b[38;5;241m+\u001b[39m ddw \u001b[38;5;28;01mfor\u001b[39;00m dw, ddw \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(del_w, delta_del_w)]\n",
      "Cell \u001b[1;32mIn[24], line 62\u001b[0m, in \u001b[0;36mDNN.backPropagation\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     58\u001b[0m outs, activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforwardPropagation(x\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m     60\u001b[0m loss \u001b[38;5;241m=\u001b[39m SigmoidCrossEntropyLoss(activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], y\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m---> 62\u001b[0m delta_cost \u001b[38;5;241m=\u001b[39m activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m y\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     64\u001b[0m delta \u001b[38;5;241m=\u001b[39m delta_cost\n\u001b[0;32m     65\u001b[0m del_b[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m delta\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,100) (0,) "
     ]
    }
   ],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 100\n",
    "output_size = 10\n",
    "\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
    "x_valid_flat = x_valid.reshape(x_valid.shape[0], -1)\n",
    "x_test_flat = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "dnn = DNN([input_size, hidden_size, output_size])\n",
    "dnn.train(x_train_flat, y_train_encoded, x_valid_flat, y_valid_encoded, epochs=100, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m valid_accuracy \u001b[38;5;241m=\u001b[39m dnn\u001b[38;5;241m.\u001b[39meval(x_valid_flat, y_valid_encoded)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m dnn\u001b[38;5;241m.\u001b[39meval(x_test_flat, y_test_encoded)\n",
      "Cell \u001b[1;32mIn[19], line 80\u001b[0m, in \u001b[0;36mDNN.eval\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, _y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, y):\n\u001b[0;32m     79\u001b[0m     outs, activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforwardPropagation(x\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(_y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     81\u001b[0m         count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(count) \u001b[38;5;241m/\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "valid_accuracy = dnn.eval(x_valid_flat, y_valid_encoded)\n",
    "print(f'Validation Accuracy: {valid_accuracy:.2f}%')\n",
    "\n",
    "test_accuracy = dnn.eval(x_test_flat, y_test_encoded)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (45000, 32, 32, 3)\n",
      "Training labels shape: (45000, 10)\n",
      "Validation data shape: (5000, 32, 32, 3)\n",
      "Validation labels shape: (5000, 10)\n",
      "Test data shape: (10000, 32, 32, 3)\n",
      "Test labels shape: (10000, 10)\n",
      "Epoch 0, Batch 0:\n",
      "batch_X shape: (100, 3072)\n",
      "batch_y shape: (0,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,100) (0,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 205\u001b[0m\n\u001b[0;32m    202\u001b[0m x_test_flat \u001b[38;5;241m=\u001b[39m x_test\u001b[38;5;241m.\u001b[39mreshape(x_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    204\u001b[0m dnn \u001b[38;5;241m=\u001b[39m DNN([input_size, hidden_size, output_size])\n\u001b[1;32m--> 205\u001b[0m dnn\u001b[38;5;241m.\u001b[39mtrain(x_train_flat, y_train_encoded, x_valid_flat, y_valid_encoded, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# 评估模型性能\u001b[39;00m\n\u001b[0;32m    208\u001b[0m valid_accuracy \u001b[38;5;241m=\u001b[39m dnn\u001b[38;5;241m.\u001b[39meval(x_valid_flat, y_valid_encoded)\n",
      "Cell \u001b[1;32mIn[32], line 123\u001b[0m, in \u001b[0;36mDNN.train\u001b[1;34m(self, X, y, x_val, y_val, batch_size, learning_rate, epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m del_b \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mzeros(b\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases]\n\u001b[0;32m    122\u001b[0m del_w \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mzeros(w\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights]\n\u001b[1;32m--> 123\u001b[0m loss, delta_del_b, delta_del_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackPropagation(batch_X, batch_y)\n\u001b[0;32m    124\u001b[0m del_b \u001b[38;5;241m=\u001b[39m [db \u001b[38;5;241m+\u001b[39m ddb \u001b[38;5;28;01mfor\u001b[39;00m db, ddb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(del_b, delta_del_b)]\n\u001b[0;32m    125\u001b[0m del_w \u001b[38;5;241m=\u001b[39m [dw \u001b[38;5;241m+\u001b[39m ddw \u001b[38;5;28;01mfor\u001b[39;00m dw, ddw \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(del_w, delta_del_w)]\n",
      "Cell \u001b[1;32mIn[32], line 145\u001b[0m, in \u001b[0;36mDNN.backPropagation\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    141\u001b[0m outs, activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforwardPropagation(x\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m    143\u001b[0m loss \u001b[38;5;241m=\u001b[39m SigmoidCrossEntropyLoss(activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], y\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m--> 145\u001b[0m delta_cost \u001b[38;5;241m=\u001b[39m activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m y\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    147\u001b[0m delta \u001b[38;5;241m=\u001b[39m delta_cost\n\u001b[0;32m    148\u001b[0m del_b[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m delta\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,100) (0,) "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_cifar10_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f, encoding='bytes')\n",
    "        X = datadict[b'data']\n",
    "        Y = datadict[b'labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def load_cifar10(ROOT):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1, 6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b,))\n",
    "        X, Y = load_cifar10_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_cifar10_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "ROOT = './cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_cifar10(ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3021197830912525, Accuracy: 0.09232\n",
      "Test Accuracy: 0.1383\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_cifar10_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f, encoding='bytes')\n",
    "        X = datadict[b'data']\n",
    "        Y = datadict[b'labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def load_cifar10(ROOT):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1, 6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b,))\n",
    "        X, Y = load_cifar10_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_cifar10_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "ROOT = './cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_cifar10(ROOT)\n",
    "\n",
    "class FullyConnectedNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z))\n",
    "        return expZ / expZ.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.A1 = self.relu(self.Z1)\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = self.softmax(self.Z2)\n",
    "        return self.A2\n",
    "\n",
    "    def compute_loss(self, Y, Y_hat):\n",
    "        m = Y.shape[0]\n",
    "        log_likelihood = -np.log(Y_hat[range(m), Y])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "\n",
    "    def backward(self, X, Y, Y_hat):\n",
    "        m = X.shape[0]\n",
    "        dZ2 = Y_hat\n",
    "        dZ2[range(m), Y] -= 1\n",
    "        dZ2 /= m\n",
    "\n",
    "        dW2 = np.dot(self.A1.T, dZ2)\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        dZ1 = dA1 * (self.Z1 > 0)\n",
    "\n",
    "        dW1 = np.dot(X.T, dZ1)\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "\n",
    "    def compute_accuracy(self, X, Y):\n",
    "        Y_hat = self.forward(X)\n",
    "        predictions = np.argmax(Y_hat, axis=1)\n",
    "        accuracy = np.mean(predictions == Y)\n",
    "        return accuracy\n",
    "\n",
    "    def train(self, X, Y, epochs=1000, learning_rate=0.01):\n",
    "        for i in range(epochs):\n",
    "            Y_hat = self.forward(X)\n",
    "            loss = self.compute_loss(Y, Y_hat)\n",
    "            self.backward(X, Y, Y_hat)\n",
    "            accuracy = self.compute_accuracy(X, Y)\n",
    "            if i % 100 == 0:\n",
    "                print(f'Epoch {i}, Loss: {loss}, Accuracy: {accuracy}')\n",
    "\n",
    "# Preprocess data\n",
    "X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
    "X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 3072  # 32*32*3\n",
    "hidden_size = 100\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Initialize and train the model\n",
    "model = FullyConnectedNN(input_size, hidden_size, output_size)\n",
    "model.train(X_train, y_train, epochs, learning_rate)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = np.argmax(model.forward(X_test), axis=1)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
