{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Train images shape: (49000, 3, 32, 32)\n",
      "Train labels shape: (49000,)\n",
      "Validation images shape: (1000, 3, 32, 32)\n",
      "Validation labels shape: (1000,)\n",
      "Test images shape: (10000, 3, 32, 32)\n",
      "Test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    \"\"\" Load single batch of CIFAR \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f, encoding='bytes')\n",
    "        X = datadict[b'data']\n",
    "        Y = datadict[b'labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def load_cifar():\n",
    "    X_train, y_train = [], []\n",
    "    for batch in range(1, 6):\n",
    "        X_batch, y_batch = load_CIFAR_batch(os.path.join('cifar-10-batches-py', 'data_batch_%d' % batch))\n",
    "        X_train.append(X_batch)\n",
    "        y_train.append(y_batch)\n",
    "    X_train = np.concatenate(X_train)\n",
    "    y_train = np.concatenate(y_train)\n",
    "    X_test, y_test = load_CIFAR_batch(os.path.join('cifar-10-batches-py', 'test_batch'))\n",
    "\n",
    "    indices = np.random.permutation(X_train.shape[0])\n",
    "    training_idx, validation_idx = indices[:49000], indices[49000:]\n",
    "    X_train, X_val = X_train[training_idx, :], X_train[validation_idx, :]\n",
    "    y_train, y_val = y_train[training_idx], y_train[validation_idx]\n",
    "\n",
    "    return {\n",
    "        'train_images': X_train,\n",
    "        'train_labels': y_train,\n",
    "        'validation_images': X_val,\n",
    "        'validation_labels': y_val,\n",
    "        'test_images': X_test,\n",
    "        'test_labels': y_test\n",
    "    }\n",
    "\n",
    "# 加载 CIFAR 数据集\n",
    "cifar_data = load_cifar()\n",
    "print(\"Data loaded successfully.\")\n",
    "print(\"Train images shape:\", cifar_data['train_images'].shape)\n",
    "print(\"Train labels shape:\", cifar_data['train_labels'].shape)\n",
    "print(\"Validation images shape:\", cifar_data['validation_images'].shape)\n",
    "print(\"Validation labels shape:\", cifar_data['validation_labels'].shape)\n",
    "print(\"Test images shape:\", cifar_data['test_images'].shape)\n",
    "print(\"Test labels shape:\", cifar_data['test_labels'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax output: [0.26030255 0.38832577 0.35137169]\n",
      "Cross entropy loss: [1.34591068 0.94591068 1.04591068]\n",
      "Regularized cross entropy loss: 1.8125773500084779\n",
      "Leaky ReLU output: [0.1 0.5 0.4]\n",
      "Leaky ReLU derivative: [1. 1. 1.]\n",
      "Learning rate at iteration 0: 0.1\n",
      "Learning rate at iteration 10000: 0.1\n",
      "Learning rate at iteration 20000: 0.010000000000000002\n",
      "Learning rate at iteration 40000: 0.001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "\n",
    "def cross_entropy(x):\n",
    "    # 添加一个小的常数以避免log(0)\n",
    "    return -np.log(x + 1e-15)\n",
    "\n",
    "def regularized_cross_entropy(layers, lam, x):\n",
    "    loss = np.mean(cross_entropy(x))\n",
    "    for layer in layers:\n",
    "        weights = layer.get_weights()\n",
    "        loss += lam * np.sum(weights ** 2)\n",
    "    return loss\n",
    "\n",
    "def leakyReLU(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leakyReLU_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "def lr_schedule(learning_rate, iteration):\n",
    "    if iteration <= 10000:\n",
    "        return learning_rate\n",
    "    elif iteration <= 30000:\n",
    "        return learning_rate * 0.1\n",
    "    else:\n",
    "        return learning_rate * 0.01\n",
    "\n",
    "# 示例使用\n",
    "class DummyLayer:\n",
    "    def get_weights(self):\n",
    "        # 返回一个示例权重矩阵\n",
    "        return np.array([1.0, 2.0, 3.0])\n",
    "\n",
    "layers = [DummyLayer() for _ in range(5)]\n",
    "lam = 0.01\n",
    "x = np.array([0.1, 0.5, 0.4])\n",
    "\n",
    "# Softmax测试\n",
    "output = softmax(x)\n",
    "print(\"Softmax output:\", output)\n",
    "\n",
    "# 交叉熵测试\n",
    "loss = cross_entropy(output)\n",
    "print(\"Cross entropy loss:\", loss)\n",
    "\n",
    "# 正则化交叉熵测试\n",
    "reg_loss = regularized_cross_entropy(layers, lam, output)\n",
    "print(\"Regularized cross entropy loss:\", reg_loss)\n",
    "\n",
    "# Leaky ReLU测试\n",
    "relu_output = leakyReLU(x)\n",
    "print(\"Leaky ReLU output:\", relu_output)\n",
    "\n",
    "# Leaky ReLU导数测试\n",
    "relu_derivative = leakyReLU_derivative(x)\n",
    "print(\"Leaky ReLU derivative:\", relu_derivative)\n",
    "\n",
    "# 学习率调度测试\n",
    "for i in [0, 10000, 20000, 40000]:\n",
    "    lr = lr_schedule(0.1, i)\n",
    "    print(f\"Learning rate at iteration {i}:\", lr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
