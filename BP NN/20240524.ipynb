{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集图像形状: (49000, 3, 32, 32)\n",
      "训练集标签形状: (49000,)\n",
      "验证集图像形状: (1000, 3, 32, 32)\n",
      "验证集标签形状: (1000,)\n",
      "测试集图像形状: (10000, 3, 32, 32)\n",
      "测试集标签形状: (10000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HUAWEI\\AppData\\Local\\Temp\\ipykernel_2668\\2088640799.py:111: RuntimeWarning: divide by zero encountered in log\n",
      "  correct_logprobs = -np.log(y_pred[range(len(y_true)), np.argmax(y_true, axis=1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 2.4796, val_loss: inf, val_acc: 0.0940\n",
      "Epoch [1], train_loss: inf, val_loss: inf, val_acc: 0.1050\n",
      "Epoch [2], train_loss: inf, val_loss: inf, val_acc: 0.1000\n",
      "Epoch [3], train_loss: inf, val_loss: inf, val_acc: 0.0890\n",
      "Epoch [4], train_loss: inf, val_loss: inf, val_acc: 0.0890\n",
      "Epoch [5], train_loss: inf, val_loss: inf, val_acc: 0.0930\n",
      "Epoch [6], train_loss: inf, val_loss: inf, val_acc: 0.0890\n",
      "Epoch [7], train_loss: inf, val_loss: inf, val_acc: 0.1070\n",
      "Epoch [8], train_loss: inf, val_loss: inf, val_acc: 0.1150\n",
      "Epoch [9], train_loss: inf, val_loss: inf, val_acc: 0.0940\n",
      "Epoch [10], train_loss: inf, val_loss: inf, val_acc: 0.1070\n",
      "Epoch [11], train_loss: inf, val_loss: inf, val_acc: 0.0960\n",
      "Epoch [12], train_loss: inf, val_loss: inf, val_acc: 0.0940\n",
      "Epoch [13], train_loss: inf, val_loss: inf, val_acc: 0.1070\n",
      "Epoch [14], train_loss: inf, val_loss: inf, val_acc: 0.0940\n",
      "Epoch [15], train_loss: inf, val_loss: inf, val_acc: 0.1150\n",
      "Epoch [16], train_loss: inf, val_loss: inf, val_acc: 0.1070\n",
      "Epoch [17], train_loss: inf, val_loss: inf, val_acc: 0.1050\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# 定义加载 CIFAR-10 数据集的函数\n",
    "def load_cifar10_batch(filename):\n",
    "    \"\"\"加载 CIFAR-10 单个批次\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f, encoding='bytes')\n",
    "        X = datadict[b'data']\n",
    "        Y = datadict[b'labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def load_cifar10(ROOT):\n",
    "    \"\"\"加载整个 CIFAR-10 数据集\"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1, 6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % b)\n",
    "        X, Y = load_cifar10_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    X_train = np.concatenate(xs)\n",
    "    y_train = np.concatenate(ys)\n",
    "    X_test, y_test = load_cifar10_batch(os.path.join(ROOT, 'test_batch'))\n",
    "\n",
    "    # 数据归一化到 [0, 1]\n",
    "    X_train /= 255.0\n",
    "    X_test /= 255.0\n",
    "\n",
    "    # 划分训练集和验证集\n",
    "    indices = np.random.permutation(X_train.shape[0])\n",
    "    training_idx, validation_idx = indices[:49000], indices[49000:]\n",
    "    X_train, X_val = X_train[training_idx, :], X_train[validation_idx, :]\n",
    "    y_train, y_val = y_train[training_idx], y_train[validation_idx]\n",
    "\n",
    "    return {\n",
    "        'train_images': X_train,\n",
    "        'train_labels': y_train,\n",
    "        'validation_images': X_val,\n",
    "        'validation_labels': y_val,\n",
    "        'test_images': X_test,\n",
    "        'test_labels': y_test\n",
    "    }\n",
    "\n",
    "# 使用示例\n",
    "cifar10_dir = 'cifar-10-batches-py'\n",
    "data = load_cifar10(cifar10_dir)\n",
    "\n",
    "X_train = data['train_images']\n",
    "y_train = data['train_labels']\n",
    "X_val = data['validation_images']\n",
    "y_val = data['validation_labels']\n",
    "X_test = data['test_images']\n",
    "y_test = data['test_labels']\n",
    "\n",
    "# 打印数据形状\n",
    "print(\"训练集图像形状:\", X_train.shape)\n",
    "print(\"训练集标签形状:\", y_train.shape)\n",
    "print(\"验证集图像形状:\", X_val.shape)\n",
    "print(\"验证集标签形状:\", y_val.shape)\n",
    "print(\"测试集图像形状:\", X_test.shape)\n",
    "print(\"测试集标签形状:\", y_test.shape)\n",
    "\n",
    "# 对标签进行 One-hot 编码\n",
    "def one_hot_encode(y, num_classes):\n",
    "    encoded = np.zeros((len(y), num_classes))\n",
    "    for idx, val in enumerate(y):\n",
    "        encoded[idx, val] = 1\n",
    "    return encoded\n",
    "\n",
    "num_classes = 10\n",
    "y_train_encoded = one_hot_encode(y_train, num_classes)\n",
    "y_val_encoded = one_hot_encode(y_val, num_classes)\n",
    "y_test_encoded = one_hot_encode(y_test, num_classes)\n",
    "\n",
    "# 定义一个多层全连接神经网络类\n",
    "class MultiLayerNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        # 初始化权重和偏置\n",
    "        self.layers = len(hidden_sizes) + 1\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "\n",
    "        # 输入层到第一个隐藏层\n",
    "        self.W.append(np.random.randn(input_size, hidden_sizes[0]) * np.sqrt(2. / input_size))\n",
    "        self.b.append(np.zeros((1, hidden_sizes[0])))\n",
    "\n",
    "        # 隐藏层之间\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            self.W.append(np.random.randn(hidden_sizes[i-1], hidden_sizes[i]) * np.sqrt(2. / hidden_sizes[i-1]))\n",
    "            self.b.append(np.zeros((1, hidden_sizes[i])))\n",
    "\n",
    "        # 最后一个隐藏层到输出层\n",
    "        self.W.append(np.random.randn(hidden_sizes[-1], output_size) * np.sqrt(2. / hidden_sizes[-1]))\n",
    "        self.b.append(np.zeros((1, output_size)))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x <= 0, 0, 1)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        correct_logprobs = -np.log(y_pred[range(len(y_true)), np.argmax(y_true, axis=1)])\n",
    "        return np.sum(correct_logprobs) / len(y_true)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        a = x\n",
    "        for i in range(self.layers - 1):\n",
    "            z = np.dot(a, self.W[i]) + self.b[i]\n",
    "            a = self.relu(z)\n",
    "        z = np.dot(a, self.W[-1]) + self.b[-1]\n",
    "        a = self.softmax(z)\n",
    "        return a\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.argmax(self.predict_proba(x), axis=1)\n",
    "\n",
    "    def train(self, x_train, y_train, x_valid, y_valid, epochs=100, learning_rate=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            a = x_train\n",
    "            activations = [a]\n",
    "            zs = []\n",
    "            for i in range(self.layers - 1):\n",
    "                z = np.dot(a, self.W[i]) + self.b[i]\n",
    "                a = self.relu(z)\n",
    "                zs.append(z)\n",
    "                activations.append(a)\n",
    "            z = np.dot(a, self.W[-1]) + self.b[-1]\n",
    "            a = self.softmax(z)\n",
    "            zs.append(z)\n",
    "            activations.append(a)\n",
    "\n",
    "            # Compute loss\n",
    "            train_loss = self.compute_loss(y_train, a)\n",
    "            \n",
    "            # Backward pass\n",
    "            delta = a - y_train\n",
    "            dW = [np.dot(activations[-2].T, delta)]\n",
    "            db = [np.sum(delta, axis=0, keepdims=True)]\n",
    "            \n",
    "            for i in range(2, self.layers + 1):\n",
    "                delta = np.dot(delta, self.W[-i + 1].T) * self.relu_derivative(zs[-i])\n",
    "                dW.append(np.dot(activations[-i - 1].T, delta))\n",
    "                db.append(np.sum(delta, axis=0, keepdims=True))\n",
    "\n",
    "            dW.reverse()\n",
    "            db.reverse()\n",
    "\n",
    "            # Update weights\n",
    "            for i in range(self.layers):\n",
    "                self.W[i] -= learning_rate * dW[i]\n",
    "                self.b[i] -= learning_rate * db[i]\n",
    "\n",
    "            # Validate the model\n",
    "            val_proba = self.predict_proba(x_valid)\n",
    "            val_loss = self.compute_loss(y_valid, val_proba)\n",
    "            val_predictions = np.argmax(val_proba, axis=1)\n",
    "            val_accuracy = np.mean(val_predictions == np.argmax(y_valid, axis=1))\n",
    "            \n",
    "            # Print the results for the current epoch\n",
    "            print(f'Epoch [{epoch}], train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_accuracy:.4f}')\n",
    "\n",
    "# 定义和训练神经网络\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_sizes = [1024, 512, 256, 128]  # 多个隐藏层的大小\n",
    "output_size = 10\n",
    "\n",
    "nn = MultiLayerNeuralNetwork(input_size, hidden_sizes, output_size)\n",
    "x_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "x_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "\n",
    "nn.train(x_train_flat, y_train_encoded, x_val_flat, y_val_encoded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
