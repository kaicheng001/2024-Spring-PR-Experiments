{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'. \n\n------------------------------------------------------------------\n- Dataset Pipeline Error Message: \n------------------------------------------------------------------\n[ERROR] Invalid cifar folder, cifar(.bin) files are missing under ./cifar-10-batches-py.\n\n------------------------------------------------------------------\n- C++ Call Stack: (For framework developers) \n------------------------------------------------------------------\nmindspore\\ccsrc\\minddata\\dataset\\engine\\datasetops\\source\\cifar_op.cc(223).\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 68\u001b[0m\n\u001b[0;32m     65\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m create_dataset(data_path, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# 评估模型\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(test_dataset, net):\n",
      "Cell \u001b[1;32mIn[3], line 57\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(train_dataset, net, loss_fn, optim, epochs)\u001b[0m\n\u001b[0;32m     54\u001b[0m train_network\u001b[38;5;241m.\u001b[39mset_train()\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_dataset\u001b[38;5;241m.\u001b[39mcreate_dict_iterator():\n\u001b[0;32m     58\u001b[0m         images \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     59\u001b[0m         labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\anaconda\\envs\\mindspore\\lib\\site-packages\\mindspore\\dataset\\engine\\iterators.py:152\u001b[0m, in \u001b[0;36mIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterator does not have a running C++ pipeline.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Note offload is applied inside _get_next() if applicable since get_next converts to output format\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\mindspore\\lib\\site-packages\\mindspore\\dataset\\engine\\iterators.py:277\u001b[0m, in \u001b[0;36mDictIterator._get_next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    275\u001b[0m     logger\u001b[38;5;241m.\u001b[39mcritical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory error occurred, process will exit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    276\u001b[0m     os\u001b[38;5;241m.\u001b[39mkill(os\u001b[38;5;241m.\u001b[39mgetpid(), signal\u001b[38;5;241m.\u001b[39mSIGKILL)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[1;32md:\\anaconda\\envs\\mindspore\\lib\\site-packages\\mindspore\\dataset\\engine\\iterators.py:260\u001b[0m, in \u001b[0;36mDictIterator._get_next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffload_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_md_to_output(t) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetNextAsMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    261\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_md_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\u001b[38;5;241m.\u001b[39mGetNextAsList()]\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'. \n\n------------------------------------------------------------------\n- Dataset Pipeline Error Message: \n------------------------------------------------------------------\n[ERROR] Invalid cifar folder, cifar(.bin) files are missing under ./cifar-10-batches-py.\n\n------------------------------------------------------------------\n- C++ Call Stack: (For framework developers) \n------------------------------------------------------------------\nmindspore\\ccsrc\\minddata\\dataset\\engine\\datasetops\\source\\cifar_op.cc(223).\n\n\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "from mindspore import Tensor, nn, context\n",
    "from mindspore.dataset import vision, transforms\n",
    "from mindspore.dataset.vision import Inter\n",
    "from mindspore.train.callback import LossMonitor\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"CPU\")\n",
    "\n",
    "# 加载CIFAR-10数据集\n",
    "def create_dataset(data_path, batch_size=32, repeat_size=1, training=True):\n",
    "    cifar10_ds = mindspore.dataset.Cifar10Dataset(data_path)\n",
    "\n",
    "    trans = [\n",
    "        vision.Resize((32, 32), interpolation=Inter.LINEAR),\n",
    "        vision.Rescale(1.0 / 255.0, 0.0),\n",
    "        vision.HWC2CHW()\n",
    "    ]\n",
    "\n",
    "    if training:\n",
    "        trans += [vision.RandomCrop((32, 32), (4, 4, 4, 4)), vision.RandomHorizontalFlip()]\n",
    "\n",
    "    trans += [transforms.TypeCast(mindspore.float32)]\n",
    "    cifar10_ds = cifar10_ds.map(operations=trans, input_columns=\"image\")\n",
    "    cifar10_ds = cifar10_ds.batch(batch_size, drop_remainder=True)\n",
    "    cifar10_ds = cifar10_ds.repeat(repeat_size)\n",
    "\n",
    "    return cifar10_ds\n",
    "\n",
    "# 定义神经网络模型\n",
    "class SimpleNN(nn.Cell):\n",
    "    def __init__(self, num_class=10, num_channel=3):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Dense(num_channel * 32 * 32, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Dense(512, num_class)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "net = SimpleNN(num_class=10, num_channel=3)\n",
    "loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "optim = nn.Adam(net.trainable_params(), learning_rate=0.001)\n",
    "\n",
    "# 训练模型\n",
    "def train_model(train_dataset, net, loss_fn, optim, epochs=10):\n",
    "    net_with_loss = nn.WithLossCell(net, loss_fn)\n",
    "    train_network = nn.TrainOneStepCell(net_with_loss, optim)\n",
    "    train_network.set_train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for data in train_dataset.create_dict_iterator():\n",
    "            images = data[\"image\"]\n",
    "            labels = data[\"label\"]\n",
    "            loss = train_network(images, labels)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss}\")\n",
    "\n",
    "# 加载数据集\n",
    "data_path = \"./cifar-10-batches-py\"\n",
    "train_dataset = create_dataset(data_path, batch_size=32, training=True)\n",
    "\n",
    "# 训练模型\n",
    "train_model(train_dataset, net, loss_fn, optim, epochs=10)\n",
    "\n",
    "# 评估模型\n",
    "def evaluate_model(test_dataset, net):\n",
    "    net.set_train(False)\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data in test_dataset.create_dict_iterator():\n",
    "        images = data[\"image\"]\n",
    "        labels = data[\"label\"]\n",
    "        outputs = net(images)\n",
    "        predictions = outputs.argmax(axis=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += (predictions == labels).sum().asnumpy()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 加载测试集\n",
    "test_dataset = create_dataset(data_path, batch_size=32, training=False)\n",
    "\n",
    "# 评估模型\n",
    "evaluate_model(test_dataset, net)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
