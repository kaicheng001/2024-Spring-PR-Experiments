{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T18:26:25.787403Z",
     "iopub.status.busy": "2021-02-26T18:26:25.786736Z",
     "iopub.status.idle": "2021-02-26T18:26:25.790112Z",
     "shell.execute_reply": "2021-02-26T18:26:25.789426Z"
    },
    "papermill": {
     "duration": 0.018247,
     "end_time": "2021-02-26T18:26:25.790323",
     "exception": false,
     "start_time": "2021-02-26T18:26:25.772076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T18:26:25.811761Z",
     "iopub.status.busy": "2021-02-26T18:26:25.811043Z",
     "iopub.status.idle": "2021-02-26T18:26:25.815831Z",
     "shell.execute_reply": "2021-02-26T18:26:25.816410Z"
    },
    "papermill": {
     "duration": 0.016788,
     "end_time": "2021-02-26T18:26:25.816591",
     "exception": false,
     "start_time": "2021-02-26T18:26:25.799803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = [\"data/cifar10/train/data_batch_1.bin\",\"data/cifar10/train/data_batch_2.bin\",\"data/cifar10/train/data_batch_3.bin\",\"data/cifar10/train/data_batch_4.bin\",\"data/cifar10/train/data_batch_5.bin\"]\n",
    "# files = [\"cifar-10-batches-py/data_batch_1.bin\",\"cifar-10-batches-py/data_batch_2.bin\",\"cifar-10-batches-py/data_batch_3.bin\",\"cifar-10-batches-py/data_batch_4.bin\",\"cifar-10-batches-py/data_batch_5.bin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T18:26:25.837803Z",
     "iopub.status.busy": "2021-02-26T18:26:25.837092Z",
     "iopub.status.idle": "2021-02-26T18:26:25.841693Z",
     "shell.execute_reply": "2021-02-26T18:26:25.841168Z"
    },
    "papermill": {
     "duration": 0.016468,
     "end_time": "2021-02-26T18:26:25.841843",
     "exception": false,
     "start_time": "2021-02-26T18:26:25.825375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = np.zeros((50000, 3072))\n",
    "y_train = np.zeros((50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T18:26:25.866382Z",
     "iopub.status.busy": "2021-02-26T18:26:25.865707Z",
     "iopub.status.idle": "2021-02-26T18:26:30.333157Z",
     "shell.execute_reply": "2021-02-26T18:26:30.333693Z"
    },
    "papermill": {
     "duration": 4.48296,
     "end_time": "2021-02-26T18:26:30.333866",
     "exception": false,
     "start_time": "2021-02-26T18:26:25.850906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x06'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fo:\n\u001b[1;32m---> 22\u001b[0m         \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(fo, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbytes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m         start \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m     24\u001b[0m         end \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10000\u001b[39m\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '\\x06'."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# 确保路径正确\n",
    "files = [\n",
    "    \"data/cifar10/train/data_batch_1.bin\",\n",
    "    \"data/cifar10/train/data_batch_2.bin\",\n",
    "    \"data/cifar10/train/data_batch_3.bin\",\n",
    "    \"data/cifar10/train/data_batch_4.bin\",\n",
    "    \"data/cifar10/train/data_batch_5.bin\"\n",
    "]\n",
    "\n",
    "# 初始化数组\n",
    "x_train = np.zeros((50000, 3072), dtype=np.uint8)\n",
    "y_train = np.zeros((50000,), dtype=np.int32)\n",
    "\n",
    "# 加载数据\n",
    "i = 0\n",
    "for file in files:\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "        start = i * 10000\n",
    "        end = start + 10000\n",
    "        x_train[start:end, :] = dict[b'data']\n",
    "        y_train[start:end] = dict[b'labels']\n",
    "        print(start, end)\n",
    "        i += 1\n",
    "\n",
    "print(\"Data loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T18:26:30.360879Z",
     "iopub.status.busy": "2021-02-26T18:26:30.360192Z",
     "iopub.status.idle": "2021-02-26T18:26:30.363558Z",
     "shell.execute_reply": "2021-02-26T18:26:30.363052Z"
    },
    "papermill": {
     "duration": 0.018703,
     "end_time": "2021-02-26T18:26:30.363701",
     "exception": false,
     "start_time": "2021-02-26T18:26:30.344998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_test = np.zeros((10000,3072))\n",
    "y_test = np.zeros((10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T18:26:30.392996Z",
     "iopub.status.busy": "2021-02-26T18:26:30.392038Z",
     "iopub.status.idle": "2021-02-26T18:26:30.397451Z",
     "shell.execute_reply": "2021-02-26T18:26:30.396929Z"
    },
    "papermill": {
     "duration": 0.023107,
     "end_time": "2021-02-26T18:26:30.397619",
     "exception": false,
     "start_time": "2021-02-26T18:26:30.374512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T18:26:30.426850Z",
     "iopub.status.busy": "2021-02-26T18:26:30.426186Z",
     "iopub.status.idle": "2021-02-26T18:26:30.923681Z",
     "shell.execute_reply": "2021-02-26T18:26:30.922997Z"
    },
    "papermill": {
     "duration": 0.514743,
     "end_time": "2021-02-26T18:26:30.923850",
     "exception": false,
     "start_time": "2021-02-26T18:26:30.409107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x03'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/cifar10/test/test_batch.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fo:\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(fo, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbytes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     x_test[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m10000\u001b[39m,:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m     y_test[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m10000\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '\\x03'."
     ]
    }
   ],
   "source": [
    "\n",
    "file = \"data/cifar10/test/test_batch.bin\"\n",
    "with open(file, 'rb') as fo:\n",
    "    dict = pickle.load(fo, encoding='bytes')\n",
    " \n",
    "    x_test[0:10000,:] = dict[b'data']\n",
    "    y_test[0:10000] = dict[b'labels']\n",
    "    \n",
    "    fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T18:26:30.966804Z",
     "iopub.status.busy": "2021-02-26T18:26:30.965891Z",
     "iopub.status.idle": "2021-02-26T18:26:30.969926Z",
     "shell.execute_reply": "2021-02-26T18:26:30.969252Z"
    },
    "papermill": {
     "duration": 0.034081,
     "end_time": "2021-02-26T18:26:30.970078",
     "exception": false,
     "start_time": "2021-02-26T18:26:30.935997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unpickle(fileName):\n",
    "    '''\n",
    "    Description: retrieve data from CIFAR-10 Pickles\n",
    "    Params: fileName = filename to unpickle\n",
    "    Outputs: Unpickled Dict\n",
    "    '''\n",
    "    with open(fileName, 'rb') as f:\n",
    "        dict = pickle.load(f, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "\n",
    "def merge_batches():\n",
    "    \n",
    "    for i in range(0,5):\n",
    "        fileName = \"../input/cifar10/data_batch_\" + str(i + 1)\n",
    "        data = unpickle(fileName)\n",
    "        if i == 0:\n",
    "            features = data[b'data']\n",
    "            labels = np.array(data[b'labels'])\n",
    "        else:\n",
    "            features = np.append(features, data[b'data'], axis=0)\n",
    "            labels = np.append(labels, data[b'labels'], axis=0)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def one_hot_encode(data):\n",
    "    '''\n",
    "    Description: Encode Target Label IDs to one hot vector of size L where L is the\n",
    "    number of unique labels\n",
    "    Params: data = list of label IDs\n",
    "    Outputs: List of One Hot Vectors\n",
    "    '''\n",
    "    one_hot = np.zeros((data.shape[0], 10))\n",
    "    one_hot[np.arange(data.shape[0]), data] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def normalize(data):\n",
    "    '''\n",
    "    Description: Normalize Pixel values\n",
    "    Params: list of Image Pixel Features\n",
    "    Outputs: Normalized Image Pixel Features\n",
    "    '''\n",
    "    return data / 255.0\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "    '''\n",
    "    Description: helper function to load and preprocess CIFAR-10 training data batches\n",
    "    Params: num_to_load = number of batches of CIFAR-10 to load and merge\n",
    "    Outputs: Pre-processed CIFAR-10 image features and labels\n",
    "    '''\n",
    "    X, y = merge_batches()\n",
    "    X = normalize(X)\n",
    "    X = X.reshape(-1, 3072, 1)\n",
    "    y = one_hot_encode(y)\n",
    "    y = y.reshape(-1, 10, 1)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def dataset_split(X, y, ratio=0.9):\n",
    "    '''\n",
    "    Description: helper function to split training data into training and validation\n",
    "    Params: X=image features\n",
    "            y=labels\n",
    "            ratio = ratio of training data from total data\n",
    "    Outputs: training data (features and labels) and validation data\n",
    "    '''\n",
    "    split = int(ratio * X.shape[0])\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    training_idx, val_idx = indices[:split], indices[split:]\n",
    "    X_train, X_val = X[training_idx, :], X[val_idx, :]\n",
    "    y_train, y_val = y[training_idx, :], y[val_idx, :]\n",
    "    print(\"Records in Training Dataset\", X_train.shape[0])\n",
    "    print(\"Records in Validation Dataset\", X_val.shape[0])\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "def sigmoid(out):\n",
    "    '''\n",
    "    Description: Sigmoid Activation\n",
    "    Params: out = a list/matrix to perform the activation on\n",
    "    Outputs: Sigmoid activated list/matrix\n",
    "    '''\n",
    "    return 1.0 / (1.0 + np.exp(-out))\n",
    "\n",
    "\n",
    "def delta_sigmoid(out):\n",
    "    '''\n",
    "    Description: Derivative of Sigmoid Activation\n",
    "    Params: out = a list/matrix to perform the activation on\n",
    "    Outputs: Delta(Sigmoid) activated list/matrix\n",
    "    '''\n",
    "    return sigmoid(out) * (1 - sigmoid(out))\n",
    "\n",
    "def SigmoidCrossEntropyLoss(a, y):\n",
    "\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T18:26:31.022691Z",
     "iopub.status.busy": "2021-02-26T18:26:31.008044Z",
     "iopub.status.idle": "2021-02-26T18:26:31.026190Z",
     "shell.execute_reply": "2021-02-26T18:26:31.025608Z"
    },
    "papermill": {
     "duration": 0.044583,
     "end_time": "2021-02-26T18:26:31.026343",
     "exception": false,
     "start_time": "2021-02-26T18:26:30.981760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    '''\n",
    "            Description: Class to define the Deep Neural Network\n",
    "    '''\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        '''\n",
    "        Description: initialize the biases and weights using a Gaussian\n",
    "        distribution with mean 0, and variance 1.\n",
    "        Biases are not set for 1st layer that is the input layer.\n",
    "        Params: sizes = a list of size L; where L is the number of layers\n",
    "                        in the deep neural network and each element of list contains\n",
    "                        the number of neuron in that layer.\n",
    "                        first and last elements of the list corresponds to the input\n",
    "                        layer and output layer respectively\n",
    "                        intermediate layers are hidden layers.\n",
    "        '''\n",
    "        self.num_layers = len(sizes)\n",
    "        # setting appropriate dimensions for weights and biases\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def forwardPropagation(self, x):\n",
    "        '''\n",
    "        Description: Forward Passes an image feature matrix through the Deep Neural\n",
    "                                 Network Architecture.\n",
    "        Params: x = Image Features\n",
    "        Outputs: 2 lists which stores outputs and activations at every layer,\n",
    "                 1st list is non-activated and 2nd list is activated\n",
    "                 The last element of the 2nd list corresponds to the scores against\n",
    "                 10 labels in the dataset.\n",
    "        '''\n",
    "        activation = x\n",
    "        activations = [x]  # list to store activations for every layer\n",
    "        outs = []  # list to store out vectors for every layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            out = np.dot(w, activation) + b\n",
    "            outs.append(out)\n",
    "            activation = sigmoid(out)\n",
    "            activations.append(activation)\n",
    "        return outs, activations\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        '''\n",
    "        Description: A data iterator for batching of image features and labels\n",
    "        Params: X, y = lists of Features and corresponding labels, these lists\n",
    "                                   have to be batched.\n",
    "                batch_size =  size of the batch\n",
    "        Outputs: a batch of image features and labels of size = batch_size\n",
    "        '''\n",
    "        for batch_idx in range(0, X.shape[0], batch_size):\n",
    "            batch = zip(X[batch_idx:batch_idx + batch_size],\n",
    "                        y[batch_idx:batch_idx + batch_size])\n",
    "            yield batch\n",
    "\n",
    "    def train(self, X, y,x_val,y_val ,batch_size=100, learning_rate=0.01, epochs=100):\n",
    "        '''\n",
    "        Description: Batch-wise trains image features against corresponding labels.\n",
    "                     The weights and biases of the neural network are updated through\n",
    "                     backpropagation on batches using SGD\n",
    "                     del_b and del_w are of same size as all the weights and biases\n",
    "                     of all the layers. del_b and del_w contains the gradients which\n",
    "                     are used to update weights and biases\n",
    "        Params: X, y = lists of training features and corresponding labels\n",
    "                batch_size =  size of the batch\n",
    "                learning_rate = eta; controls the size of changes in weights & biases\n",
    "                epochs = no. of times to iterate of the whole data\n",
    "        '''\n",
    "        n_batches = X.shape[0] // batch_size\n",
    "        for j in range(epochs):\n",
    "            batch_iter = self.get_batch(X, y, batch_size)\n",
    "            #print(type(batch_iter))\n",
    "            for i in range(n_batches):\n",
    "                batch = next(batch_iter)\n",
    "                # same shape as self.biases\n",
    "                del_b = [np.zeros(b.shape) for b in self.biases]\n",
    "                # same shape as self.weights\n",
    "                del_w = [np.zeros(w.shape) for w in self.weights]\n",
    "                for batch_X, batch_y in batch:\n",
    "                    # accumulate all the bias and weight gradients\n",
    "                    loss, delta_del_b, delta_del_w = self.backPropagation(\n",
    "                        batch_X, batch_y)\n",
    "                    del_b = [db + ddb for db, ddb in zip(del_b, delta_del_b)]\n",
    "                    del_w = [dw + ddw for dw, ddw in zip(del_w, delta_del_w)]\n",
    "            # update weight and biases by multiplying ratio learning rate and batch_size\n",
    "            # multiplied with the accumulated gradients(partial derivatives)\n",
    "            # calculate change in weight(delta) and biases and update weight\n",
    "            # with the changes\n",
    "            self.weights = [w - (learning_rate / batch_size)\n",
    "                            * delw for w, delw in zip(self.weights, del_w)]\n",
    "            self.biases = [b - (learning_rate / batch_size)\n",
    "                           * delb for b, delb in zip(self.biases, del_b)]\n",
    "            acc_val = self.eval(x_val,y_val)\n",
    "            acc_train = self.eval(X,y)\n",
    "            \n",
    "            print(\"\\nEpoch %d complete\\t training accuracy: %f\\n\"%(j, acc_train))\n",
    "            print(\"\\nEpoch %d complete\\t validation accuracy: %f\\n\"%(j, acc_val))\n",
    "            print(\"\\nEpoch %d complete\\t taining Loss: %f\\n\"%(j, loss))\n",
    "\n",
    "    def backPropagation(self, x, y):\n",
    "\n",
    "        del_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        del_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "\n",
    "        outs, activations = self.forwardPropagation(x)\n",
    "\n",
    "        loss = SigmoidCrossEntropyLoss(activations[-1],y)\n",
    "\n",
    "        delta_cost = activations[-1] - y\n",
    "\n",
    "        delta = delta_cost\n",
    "        del_b[-1] = delta\n",
    "        del_w[-1] = np.dot(delta, activations[-2].T)\n",
    "\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            out = outs[-l]\n",
    "            delta_activation = delta_sigmoid(out)\n",
    "            \n",
    "            delta = np.dot(self.weights[-l + 1].T, delta) * delta_activation\n",
    "            del_b[-l] = delta\n",
    "            del_w[-l] = np.dot(delta, activations[-l - 1].T)\n",
    "        return (loss, del_b, del_w)\n",
    "\n",
    "    def eval(self, X, y):\n",
    "        '''\n",
    "        Description: Based on trained(updated) weights and biases, predict a label and compare\n",
    "                     it with original label and calculate accuracy\n",
    "        Params: X, y = a data example from validation dataset (image features, labels)\n",
    "        Outputs: accuracy of prediction\n",
    "        '''\n",
    "        count = 0\n",
    "        for x, _y in zip(X, y):\n",
    "            outs, activations = self.forwardPropagation(x)\n",
    "            # postion of maximum value is the predicted label\n",
    "            if np.argmax(activations[-1]) == np.argmax(_y):\n",
    "                count += 1\n",
    "        \n",
    "        \n",
    "        #print(\"Accuracy: %f\" % ((float(count) / X.shape[0]) * 100))\n",
    "        return float(count) / X.shape[0] * 100\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Description: Based on trained(updated) weights and biases, predict a label for an\n",
    "                                 image which does not have a label.\n",
    "        Params: X = list of features of unknown images\n",
    "        Outputs: list containing the predicted label for the corresponding unknown image\n",
    "        '''\n",
    "        labels = unpickle(\"cifar-10-batches-py/batches.meta\")[\"label_names\"]\n",
    "        preds = np.array([])\n",
    "        for x in X:\n",
    "            outs, activations = self.feedforward(x)\n",
    "            preds = np.append(preds, np.argmax(activations[-1]))\n",
    "        preds = np.array([labels[int(p)] for p in preds])\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.011476,
     "end_time": "2021-02-26T18:26:31.049704",
     "exception": false,
     "start_time": "2021-02-26T18:26:31.038228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T18:26:31.080329Z",
     "iopub.status.busy": "2021-02-26T18:26:31.078939Z",
     "iopub.status.idle": "2021-02-26T18:29:42.162910Z",
     "shell.execute_reply": "2021-02-26T18:29:42.163441Z"
    },
    "papermill": {
     "duration": 191.102138,
     "end_time": "2021-02-26T18:29:42.163652",
     "exception": false,
     "start_time": "2021-02-26T18:26:31.061514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/cifar10/data_batch_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X, y \u001b[38;5;241m=\u001b[39m preprocess()\n\u001b[0;32m      2\u001b[0m X_train, y_train, X_val, y_val \u001b[38;5;241m=\u001b[39m dataset_split(X, y)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m DNN([\u001b[38;5;241m3072\u001b[39m,\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m10\u001b[39m])  \u001b[38;5;66;03m# initialize the model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 53\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m():\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Description: helper function to load and preprocess CIFAR-10 training data batches\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    Params: num_to_load = number of batches of CIFAR-10 to load and merge\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    Outputs: Pre-processed CIFAR-10 image features and labels\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m merge_batches()\n\u001b[0;32m     54\u001b[0m     X \u001b[38;5;241m=\u001b[39m normalize(X)\n\u001b[0;32m     55\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3072\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 16\u001b[0m, in \u001b[0;36mmerge_batches\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     15\u001b[0m     fileName \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../input/cifar10/data_batch_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     data \u001b[38;5;241m=\u001b[39m unpickle(fileName)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     18\u001b[0m         features \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m, in \u001b[0;36munpickle\u001b[1;34m(fileName)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munpickle\u001b[39m(fileName):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Description: retrieve data from CIFAR-10 Pickles\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    Params: fileName = filename to unpickle\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    Outputs: Unpickled Dict\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fileName, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbytes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/cifar10/data_batch_1'"
     ]
    }
   ],
   "source": [
    "X, y = preprocess()\n",
    "X_train, y_train, X_val, y_val = dataset_split(X, y)\n",
    "    \n",
    "model = DNN([3072,30, 20, 10])  # initialize the model\n",
    "model.train(X_train, y_train,X_val, y_val, epochs=5)  # train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.013586,
     "end_time": "2021-02-26T18:29:42.191271",
     "exception": false,
     "start_time": "2021-02-26T18:29:42.177685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 202.548577,
   "end_time": "2021-02-26T18:29:42.815982",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-26T18:26:20.267405",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
