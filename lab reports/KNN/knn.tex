\documentclass[12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[UTF8]{ctex}
\usepackage{xeCJK}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx} % For including figures
\usepackage{amsmath}  % For math fonts, symbols and environments
\usepackage{pgfgantt} % For Gantt charts
\usepackage[hidelinks]{hyperref} % For hyperlinks
\usepackage{enumitem} % For customizing lists
\definecolor{blue}{HTML}{74BBC9}
\definecolor{yellow}{HTML}{F7E967}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tocloft} % 导入tocloft包
\usepackage{zi4}
\usepackage{fontspec}
\usepackage{setspace} % For setting line spacing

\usepackage{booktabs} % For professional looking tables
\usepackage{array}    % For extended column definitions
\usepackage{amsfonts} % For math fonts like '\mathbb{}'
\usepackage{amssymb}  % For math symbols
\usepackage{caption}  % For custom captions
\usepackage[table]{xcolor} % For coloring tables
\usepackage{tabularx} % For auto-sized table columns
\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{positioning}

% 目录标题样式定义
\renewcommand{\cfttoctitlefont}{\hfill\Large\bfseries}
\renewcommand{\cftaftertoctitle}{\hfill\mbox{}\par}

% 设置中文主字体为宋体，指定相对路径
\setCJKmainfont[
    Path = ./fonts/,
    BoldFont = SimSun.ttc,
    ItalicFont = SimSun.ttc
]{SimSun.ttc}

% 设置英文主字体为Times New Roman
\setmainfont{Times New Roman}

% 设置正文格式：宋体，小四，行距20磅
\renewcommand\normalsize{%
    \CJKfamily{song}\fontsize{12pt}{20pt}\selectfont}

% Monokai theme with a lighter background
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\setmonofont{Source Code Pro}[Contextuals={Alternate}]

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\title{\textbf{PR Experiment Report Collection}}
\author{58122204 谢兴}
\date{\today}

\begin{document}

\begin{titlepage}
  \begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.2]{figures/southeast_university_logo.png}
    % \caption{实际投掷距离与理想条件下投掷距离对比}
    \label{fig1}
  \end{figure}

  \centering
  \vspace*{40pt}
  \Huge\textbf{模式识别实验报告}

  \vspace{60pt}
  \Large
  % 专业：人工智能

  % \vspace{30pt}
  % 学号：58122204

  % \vspace{30pt}
  % 年级：大二

  % \vspace{30pt}
  % 姓名：谢兴


  % \vspace{30pt}
  % 签名：


  % \vspace{30pt}
  % 时间：\today
  \begin{center}

    % \begin{tabularx}{0.8\textwidth}{>{\raggedleft\arraybackslash}X >{\centering\arraybackslash}X}
    %   专业： & \underline{\makebox[6cm][c]{人工智能}} \\
    %   学号： & \underline{\makebox[6cm][c]{58122204}} \\
    %   年级： & \underline{\makebox[6cm][c]{大二}} \\
    %   姓名： & \underline{\makebox[6cm][c]{谢兴}} \\
    % \end{tabularx}
    \begin{table}[h]
      \centering
      \begin{Large}
        \begin{spacing}{1.5} % 设置行间距为1.5倍
          \begin{tabular}{p{1.5cm} p{6cm}<{\centering}}
            专业: & \underline{\makebox[6cm][c]{人工智能}}     \\
            学号: & \underline{\makebox[6cm][c]{58122204}} \\
            年级: & \underline{\makebox[6cm][c]{大二}}       \\
            姓名: & \underline{\makebox[6cm][c]{谢兴}}       \\
          \end{tabular}
        \end{spacing}
      \end{Large}
    \end{table}




    \vspace{5cm}

    \begin{flushright}
      \begin{tabularx}{0.4\textwidth}{>{\raggedleft\arraybackslash}X >{\centering\arraybackslash}X}
        签名： & \\
        时间： & \\
      \end{tabularx}
    \end{flushright}
  \end{center}
  %   \begin{center}
  %     \begin{tabular}{rl}
  %         专业： & \underline{\hspace{6cm}} \\
  %         学号： & \underline{\hspace{6cm}} \\
  %         年级： & \underline{\hspace{6cm}} \\
  %         姓名： & \underline{\hspace{6cm}} \\
  %     \end{tabular}
  % \end{center}

\end{titlepage}

\newpage
\tableofcontents

% 实验一
\newpage
\section{\centering 实验一 KNN Classification}

\subsection{问题描述}
\subsection{概述}
利用KNN算法，对 Iris 鸢尾花数据集中的测试集进行分类。
\subsection{任务说明}
\begin{enumerate}
  \item 利用欧式距离作为KNN算法的度量函数，对测试集
        进行分类。实验报告中，要求在验证集上分析近邻
        数$k$对KNN算法分类精度的影响。
  \item 利用马氏距离作为KNN算法的度量函数，对测试集
        进行分类。
  \item 基于MindSpore平台提供的官方模型库，对相同的
        数据集进行训练，并与自己独立实现的算法对比结
        果（包括但不限于准确率、算法迭代收敛次数等指
        标），并分析结果中出现差异的可能原因，给出使
        用MindSpore的心得和建议。
  \item （加分项）使用MindSpore平台提供的相似任务数
        据集（例如，其他的分类任务数据集）测试自己独
        立实现的算法并与MindSpore平台上的官方实现算
        法进行对比，并进一步分析差异及其成因。
\end{enumerate}

\subsection{实现步骤与流程}

\subsubsection{实验思路}
\begin{enumerate}
  \item 导入必要的库，包括numpy, pandas, matplotlib, plotly 和 seaborn；
  \item 数据加载和基本信息显示；
        \begin{enumerate}
          \item 从 data/train.csv 文件中加载训练数据集
          \item 显示数据集的前几行数据
          \item 显示数据集的描述性统计信息
          \item 显示数据集的基本信息，包括数据类型和缺失值情况
        \end{enumerate}
  \item 数据可视化；
        \begin{enumerate}
          \item 使用 seaborn 绘制数据集的特征两两关系图，并按标签着色
          \item 使用 plotly 绘制数据分布的饼图
          \item 分别绘制每个特征（萼片长度、萼片宽度、花瓣长度、花瓣宽度）的箱线图和直方图
        \end{enumerate}
  \item 实现基于Euiclidean距离和基于Mahalanobis距离的KNN算法；

  \item 数据处理和预测；
        \begin{enumerate}
          \item 加载测试数据集并进行必要的类型转换和缺失值检查
          \item 使用预训练模型对测试数据进行预测，并将预测结果保存到 CSV 文件中
        \end{enumerate}
  \item 最后对比欧式距离和马氏距离两种度量方式的分类效果和差异
        \begin{enumerate}
          \item 比较多个预测结果文件 task1\_test\_prediction.csv 和 task2\_test\_prediction.csv之间的差异，
                找出不同的行和列，并打印出不同值的位置
        \end{enumerate}
\end{enumerate}

\subsubsection{数学模型}
KNN算法的数学模型如下：

给定一个测试样本$x$，KNN算法通过计算$x$与训练集中所有样本之间的距离（常用欧氏距离），选择距离最近的$k$个样本，然后通过多数投票法决定$x$的类别。
欧氏距离的计算公式为：
\[
  d(x_1, x_2) = \sqrt{\sum_{i=1}^{n} (x_{1i} - x_{2i})^2}
\]

马氏距离的计算公式为：
\[d(x_1, x_2) = \sqrt{(x_1 - x_2)^T \Sigma^{-1} (x_1 - x_2)}\]
其中，$\Sigma$为协方差矩阵。


\subsubsection{关键难点}
\begin{enumerate}
  \item 如何高效地计算欧氏距离。
  \item 如何在较大的数据集上进行快速的邻居搜索。
        % \item 如何处理训练数据和测试数据的维度一致性问题。
        % \item 如何在分类时处理类别不均衡的问题。
\end{enumerate}


\subsubsection{算法描述}
基于Euiclidean距离和Mahalanobis距离的KNN算法的伪代码分别
见算法~\ref{K-Nearest Neighbors Based on Euclidean Distance} 和算法~\ref{K-Nearest Neighbors Based on Mahalanobis Distance}。

\begin{algorithm}
  \caption{K-Nearest Neighbors Based on Euclidean Distance}
  \label{K-Nearest Neighbors Based on Euclidean Distance}
  \begin{algorithmic}[1]
    \State 初始化KNN分类器，邻居数为$k$
    \Procedure{拟合}{X\_train, y\_train}
    \State 存储训练数据和标签
    \EndProcedure
    \Procedure{预测}{$X$}
    \For{每个测试数据$x$}
    \State 计算$x$与所有训练样本之间的欧氏距离
    \State 对距离进行排序，选择最近的$k$个邻居
    \State 对这$k$个邻居的标签进行多数投票
    \State 将多数投票结果赋予$x$
    \EndFor
    \State \Return 预测的标签
    \EndProcedure
  \end{algorithmic}
\end{algorithm}


\begin{algorithm}
  \caption{K-Nearest Neighbors Based on Mahalanobis Distance}
  \label{K-Nearest Neighbors Based on Mahalanobis Distance}
  \begin{algorithmic}[1]
    \State 初始化KNN分类器，邻居数为$k$，矩阵$A$的维度为$e$，学习率为$\eta$，最大迭代次数为$max\_iter$
    \Procedure{拟合}{X\_train, y\_train}
    \State 存储训练数据和标签
    \State 初始化矩阵$A$为随机值
    \For{迭代次数 $iteration = 1, 2, \ldots, max\_iter$}
    \State 初始化梯度矩阵$\nabla A$为零
    \For{每个训练样本 $x_i$}
    \State 获取与$x_i$同类的样本索引$same\_class\_indices$
    \For{每个同类样本 $x_j$}
    \If{$i == j$}
    \State 跳过
    \EndIf
    \State 计算$p_{ij}$值
    \State 计算样本差异$diff = x_i - x_j$
    \State 更新梯度$\nabla A += 2 \cdot p_{ij} \cdot (A \cdot diff) \cdot diff^T$
    \EndFor
    \EndFor
    \State 按学习率更新矩阵$A$: $A = A - \eta \cdot \nabla A / n$
    \EndFor
    \EndProcedure
    \Procedure{预测}{$X$}
    \For{每个测试数据$x$}
    \State 计算$x$与所有训练样本之间的马氏距离
    \State 对距离进行排序，选择最近的$k$个邻居
    \State 对这$k$个邻居的标签进行多数投票
    \State 将多数投票结果赋予$x$
    \EndFor
    \State \Return 预测的标签
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsubsection{马氏距离梯度计算公式推导}

假设我们有训练数据集$\{(x_i, y_i)\}_{i=1}^n$，其中$x_i \in \mathbb{R}^d$为样本特征，$y_i$为样本类别。为了优化马氏距离下的KNN算法，我们需要学习一个矩阵$A \in \mathbb{R}^{e \times d}$，使得同类样本之间的距离最小化。马氏距离的计算公式为：
\begin{equation}
  d_M(x_i, x_j) = \sqrt{(x_i - x_j)^\top A^\top A (x_i - x_j)}
\end{equation}

为了优化矩阵$A$，我们使用如下的目标函数：
\begin{equation}
  \mathcal{L} = \sum_{i=1}^n \sum_{j \in \mathcal{N}(i)} p_{ij} \cdot d_M^2(x_i, x_j)
\end{equation}
其中，$\mathcal{N}(i)$表示与$x_i$同类的样本索引集合，$p_{ij}$为权重，定义为：
\begin{equation}
  p_{ij} = \frac{\exp(-d_M^2(x_i, x_j))}{\sum_{k \in \mathcal{N}(i)} \exp(-d_M^2(x_i, x_k))}
\end{equation}

首先，我们对$d_M^2(x_i, x_j)$进行展开：
\begin{equation}
  d_M^2(x_i, x_j) = (x_i - x_j)^\top A^\top A (x_i - x_j)
\end{equation}

为了计算梯度$\nabla_A \mathcal{L}$，我们需要对$\mathcal{L}$关于$A$求导：
\begin{equation}
  \mathcal{L} = \sum_{i=1}^n \sum_{j \in \mathcal{N}(i)} p_{ij} (x_i - x_j)^\top A^\top A (x_i - x_j)
\end{equation}

对$A$求导时，需要使用链式法则：
\begin{equation}
  \frac{\partial \mathcal{L}}{\partial A} = \sum_{i=1}^n \sum_{j \in \mathcal{N}(i)} \left( \frac{\partial p_{ij}}{\partial A} (x_i - x_j)^\top A^\top A (x_i - x_j) + p_{ij} \frac{\partial ((x_i - x_j)^\top A^\top A (x_i - x_j))}{\partial A} \right)
\end{equation}

首先计算$p_{ij}$对$A$的导数。由于$p_{ij}$包含在指数函数内，我们得到：
\begin{equation}
  \frac{\partial p_{ij}}{\partial A} = p_{ij} \left( -\sum_{k \in \mathcal{N}(i)} p_{ik} \cdot 2(x_i - x_k)^\top A^\top \cdot (x_i - x_k) + 2(x_i - x_j)^\top A^\top \cdot (x_i - x_j) \right)
\end{equation}

然后计算$(x_i - x_j)^\top A^\top A (x_i - x_j)$对$A$的导数：
\begin{equation}
  \frac{\partial ((x_i - x_j)^\top A^\top A (x_i - x_j))}{\partial A} = 2 A (x_i - x_j) (x_i - x_j)^\top
\end{equation}

将以上结果代入梯度公式中，我们得到：
\begin{equation}
  \begin{aligned}
    \nabla_A \mathcal{L} = & \sum_{i=1}^n \sum_{j \in \mathcal{N}(i)} \left[ p_{ij} \left( -\sum_{k \in \mathcal{N}(i)} p_{ik} \cdot 2(x_i - x_k)^\top A^\top \cdot (x_i - x_k) \right. \right. \\
                           & \left. \left. + 2(x_i - x_j)^\top A^\top \cdot (x_i - x_j) \right) + 2 p_{ij} A (x_i - x_j) (x_i - x_j)^\top \right]
  \end{aligned}
\end{equation}

整理后得到最终的梯度公式：
\begin{equation}
  \nabla_A \mathcal{L} = 2 \sum_{i=1}^n \sum_{j \in \mathcal{N}(i)} p_{ij} \left[ A (x_i - x_j) (x_i - x_j)^\top - \sum_{k \in \mathcal{N}(i)} p_{ik} A (x_i - x_k) (x_i - x_k)^\top \right]
\end{equation}


\subsection{实验结果与分析}
\subsubsection{数据集的部分可视化分析}
\begin{enumerate}
  \item train.csv 文件中训练数据的pairplot图如图~\ref{fig:pairplot} 所示。
  \item 训练数据的分布情况如图~\ref{fig:pie} 所示，可以看出这三类鸢尾花的数据分布比例是不完全一致的，但三类的数据量大致相同。
        % \item
\end{enumerate}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.45]{figures/knn/pairplot.png}
  \caption{train.csv训练数据的pairplot图}
  \label{fig:pairplot}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.4]{figures/knn/distribution.png}
  \caption{train.csv训练数据分布比例}
  \label{fig:pie}
\end{figure}

\subsubsection{实验结果的分析}

\begin{enumerate}
  \item 对于基于欧氏距离的KNN算法，当$k=3$时，测试集的准确率为93.33\%；
  \item 对于基于马氏距离的KNN算法，当$k=3$时，测试集的准确率为93.33\%；
  \item 将$k$从1到30遍历，分析出基于欧氏距离的KNN算法对Iris数据集进行分类的准确率随$k$的变化情况，如图~\ref{fig:accuracy} 所示。
  \item 进一步扩大$k$的取值范围，将$k$从1遍历到100，分析出基于欧氏距离的KNN算法对Iris数据集进行分类的准确率随$k$的变化情况，
        如图~\ref{fig:accuracy2} 所示。
\end{enumerate}
从上述实验结果可以分析出，基于欧式距离的最佳$k$值为5或27或29，此时的准确率最高，为100\%，
说明$k=5$，$k=27$，$k=29$时能完全正确的将Iris数据集中三类鸢尾花进行分类。
而图~\ref{fig:accuracy} 和图~\ref{fig:accuracy2} 中的准确率曲线同时也说明了KNN算法中的$k$值选择对分类准确率的影响：
\begin{enumerate}
  \item 当$k$值较小时，模型容易受到噪声的影响，导致过拟合；
  \item 当$k$值较大时，模型容易受到样本不均衡的影响，导致欠拟合。
\end{enumerate}
而$k$值过大就相当于是对所有样本进行投票，根据本实验模型（基于欧氏距离）所得的结果发现，$k \geq 93$时，准确率降到最低的26.67\%。

同时，通过图~\ref{fig:accuracy} 和图~\ref{fig:accuracy2}，我们还发现，准确率并不是$k$变化就随着变化的，而是几近分段变化的，
这从侧面说明了本实验Iris数据集的离散属性，即不同类别的鸢尾花在特征空间中的分布是不均匀的，
同时也说明了Iris数据集三种鸢尾花的部分属性具有聚集性，
这也与图~\ref{fig:pairplot}中pairplot的小图中三种鸢尾花的属性相分离契合。

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.8]{figures/knn/accuracy.png}
  \caption{分类准确率随$k$的变化情况}
  \label{fig:accuracy}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.28]{figures/knn/accuracy2.png}
  \caption{分类准确率随$k$的变化情况}
  \label{fig:accuracy2}
\end{figure}


\subsubsection{手动实现算法的评价}
传统的KNN方法的不足之处主要包括：
\begin{enumerate}
  \item 分类速度慢：最近邻分类器是基于实例学习的懒惰学习方法，因为它是根据所给训练样本构造的分类器，是将所有训练样本首先存储起来，
        当要进行分类时，就临时进行计算处理。需要计算待分样本与训练样本库中每一个样本的相似度，才能求得与其最近的K个样本。
        对于高维样本或样本集规模较大的情况，其时间和空间复杂度较高，时间代价为$O(mn)$，其中$m$为向量空间模型空间特征维数，$n$为训练样本集大小。
  \item 样本库容量依赖性较强：对KNN算法在实际应用中的限制较大：有不少类别无法提供足够的训练样本，
        使得KNN算法所需要的相对均匀的特征空间条件无法得到满足，使得识别的误差较大。
  \item 特征作用相同：与决策树归纳方法和神经网络方法相比，传统最近邻分类器认为每个属性的作用都是相同的（赋予相同权重）。
        样本的距离是根据样本的所有特征（属性）计算的。在这些特征中，有些特征与分类是强相关的，有些特征与分类是弱相关的，
        还有一些特征（可能是大部分）与分类不相关。这样，如果在计算相似度的时候，按所有特征作用相同来计算样本相似度就会误导分类过程。
  \item K值的确定：KNN算法必须指定K值，K值选择不当则分类精度不能保证。
\end{enumerate}

KNN的改进：
对于KNN分类算法的改进方法主要可以分为加快分类速度、对训练样本库的 维护、相似度的距离公式优化和K值确定四种类型。

\begin{enumerate}
  \item 加快KNN算法的分类速度

        就学习而言，懒惰学习方法比积极学习方法要快，就计算量而言，它要比积极学习方法慢许多，
        因为懒惰学习方法在进行分类时，需要进行大量的计算。针对这一问题，到目前为止绝大多数解决方法都是基于减少样本量和加快搜索
        K个最近邻速度两个方面考虑的：
        \begin{enumerate}
          \item 浓缩训练样本

                当训练样本集中样本数量较大时，为了减小计算开销，可以对训练样本集进行编辑处理，
                即从原始训练样本集中选择最优的参考子集进行K最近邻寻找，从而减少训练样本的存储量和提高计算效率。
                这类方法主要包括Condensing算法、WilSon的Editing算法 和Devijver的MultiEdit算法，
                Kuncheva使用遗传算法在这方面也进行了一些研究  。

          \item 加快K个最近邻的搜索速度

                这类方法是通过快速搜索算法，在较短时间内找到待分类样本的K个最近邻。
                在具体进行搜索时，不要使用盲目的搜索方法，而是要采用一定的方法加快搜索 速度或减小搜索范围，
                例如可以构造交叉索引表，利用匹配成功与否的历史来修 改样本库的结构，使用样本和概念来构造层次或网络来组织训练样本。

        \end{enumerate}
  \item 相似度的距离公式的优化

        为了改变传统KNN算法中特征作用相同的缺陷，可在相似度的距离公式中给 特征赋予不同权重，
        例如在欧氏距离公式中给不同特征赋予不同权重。特征的权重一般根据各个特征在分类中的作用设定，可根据特征在整个训练 样本库中的所起的作用大小来确定权重，也可根据在训练样本的局部样本 （靠近待测试样本的样本集合）中的分类作用确定权重。


  \item 对训练样本库的维护

        对训练样本库进行维护以满足KNN算法的需要，包括对训练样本库中的样本 进行添加或删除。
        对样本库的维护并不是简单的增加删除样本，而是可采用适当 的办法来保证空间的大小，如符合某种条件的样本可以加入数据库中，
        同时可以 对数据库库中已有符合某种条件的样本进行删除。从而保证训练样本库中的样本 提供KNN算法所需要的相对均匀的特征空间。
  \item K值选择 K的选择原则一般为：
        \begin{enumerate}
          \item K的选择往往通过大量独立的测试数据、多个模型来验证最佳的选择；
          \item K值一般事先确定，也可以使用动态的，例如采用固定的距离指标，只对小于该指标的样本进行分析。
        \end{enumerate}
\end{enumerate}




\subsection{MindSpore 学习使用心得体会}
本实验中使用了mindspore内置的numpy，这个库与常规的numpy有一定区别，但是使用起来也是比较方便的。

在实验过程中，原本直接使用numpy在加载数据部分是将数据转换成numpy数组，而mindspore.numpy则是转换成mindspore.Tensor，
此外我还注意到，mindspore的数据加载和处理部分与numpy有一定的区别，需要注意数据类型的转换，如asnumpy()，
在操作过程中注意数据转换成int32类型等。

% 此外，值得注意的一个细节是，直接使用numpy获得的准确率的精度要高于mindspore.numpy的，这可能是因为mindspore.numpy的精度较低。


\subsection{代码附录（数据加载可视化展示部分，具体见knn.ipynb文件）}
\subsubsection{knn.ipynb}
\begin{lstlisting}[language=Python]
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import plotly.express as px
  import seaborn as sns
  
  iris_train = pd.read_csv("data/train.csv") 
  
  class KNN:
      def __init__(self, n_neighbors=5):
          self.n_neighbors = n_neighbors
          
      def euclidean_distance(self, x1, x2):
          return np.linalg.norm(x1 - x2)
  
      def fit(self, X_train, y_train):
          self.X_train = X_train
          self.y_train = y_train
  
      def predict(self, X):
          # Create empty array to store the predictions
          predictions = []
          # Loop over X examples
          for x in X:
              # Get prediction using the prediction helper function
              prediction = self._predict(x)
              # Append the prediction to the predictions list
              predictions.append(prediction)
          return np.array(predictions)
      
  
      def _predict(self, x):
          # Create empty array to store distances
          distances = []
          # Loop over all training examples and compute the distance between x and all the training examples 
          for x_train in self.X_train:
              distance = self.euclidean_distance(x, x_train)
              distances.append(distance)
          distances = np.array(distances)
          
          # Sort by ascendingly distance and return indices of the given n neighbours
          n_neighbors_idxs = np.argsort(distances)[: self.n_neighbors]
          
          # Get labels of n-neighbour indexes
          labels = self.y_train[n_neighbors_idxs]                  
          labels = list(labels)
          # Get the most frequent class in the array
          most_occuring_value = max(labels, key=labels.count)
          return most_occuring_value
      
  class KNN2:
      def __init__(self, n_neighbors=5, e=2, learning_rate=0.01, max_iter=100):
          self.n_neighbors = n_neighbors
          self.e = e
          self.learning_rate = learning_rate
          self.max_iter = max_iter
          self.X_train = None
          self.y_train = None
          self.A = None
          
      def fit(self, X_train, y_train):
          self.X_train = X_train
          self.y_train = y_train
          n_samples, n_features = X_train.shape
          
          # Initialize matrix A randomly
          self.A = np.random.rand(self.e, n_features)
          
          # Gradient descent to learn A
          for iteration in range(self.max_iter):
              gradient = np.zeros_like(self.A)
              for i in range(n_samples):
                  xi = X_train[i]
                  yi = y_train[i]
                  same_class_indices = np.where(y_train == yi)[0]
                  for j in same_class_indices:
                      if i == j:
                          continue
                      xj = X_train[j]
                      pij = self._compute_pij(xi, xj, i, same_class_indices)
                      diff = xi - xj
                      gradient += 2 * pij * np.outer(self.A @ diff, diff)
              
              self.A -= self.learning_rate * gradient / n_samples
      
      def _compute_pij(self, xi, xj, i, same_class_indices):
          numerator = np.exp(-self.mahalanobis_distance(xi, xj) ** 2)
          denominator = 0
          for k in same_class_indices:
              if k == i:
                  continue
              xk = self.X_train[k]
              denominator += np.exp(-self.mahalanobis_distance(xi, xk) ** 2)
          return numerator / denominator
  
      def mahalanobis_distance(self, x1, x2):
          diff = x1 - x2
          return np.sqrt(np.dot(np.dot(diff, self.A.T @ self.A), diff))
  
      def predict(self, X):
          # Create empty array to store the predictions
          predictions = []
          # Loop over X examples
          for x in X:
              # Get prediction using the prediction helper function
              prediction = self._predict(x)
              # Append the prediction to the predictions list
              predictions.append(prediction)
          return np.array(predictions)
      
      def _predict(self, x):
          # Create empty array to store distances
          distances = []
          # Loop over all training examples and compute the distance between x and all the training examples 
          for x_train in self.X_train:
              distance = self.mahalanobis_distance(x, x_train)
              distances.append(distance)
          distances = np.array(distances)
          
          # Sort by ascendingly distance and return indices of the given n neighbours
          n_neighbors_idxs = np.argsort(distances)[:self.n_neighbors]
          
          # Get labels of n-neighbour indexes
          labels = self.y_train[n_neighbors_idxs]
          labels = list(labels)
          # Get the most frequent class in the array
          most_occuring_value = max(labels, key=labels.count)
          return most_occuring_value
  
  # 读取训练集
  train_data = pd.read_csv('data/train.csv')
  X_train = train_data[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']]
  y_train = train_data['label']
  
  # 读取验证集
  test_data = pd.read_csv('data/val.csv')
  X_test = test_data[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']]
  y_test = test_data['label']
  
  # 检查数据类型并转换为浮点型
  X_train = X_train.astype(float)
  X_test = X_test.astype(float)
  
  # 确保数据为NumPy数组
  X_train = X_train.values
  X_test = X_test.values
  
  # 检查y_train和y_test是否为整数型
  y_train = y_train.astype(int)
  y_test = y_test.astype(int)
  
  # 确保y_train和y_test为NumPy数组
  y_train = y_train.values
  y_test = y_test.values
  
  model = KNN(3)
  model.fit(X_train, y_train)
  model2 =KNN2(3)
  model2.fit(X_train, y_train)
  
  def compute_accuracy(y_true, y_pred):
      """
      Computes the accuracy of a classification model.
  
      Parameters:
      y_true (numpy array): A numpy array of true labels for each data point.
      y_pred (numpy array): A numpy array of predicted labels for each data point.
  
      Returns:
      float: The accuracy of the model, expressed as a percentage.
      """
      y_true = y_true.flatten()
      total_samples = len(y_true)
      correct_predictions = np.sum(y_true == y_pred)
      return (correct_predictions / total_samples) 
  
  predictions = model.predict(X_test)
  accuracy = compute_accuracy(y_test, predictions)
  print(f" our model got accuracy score of : {accuracy}")   
  
  
  # 初始化列表
  a_index = list(range(1, 31))
  accuracies = []
  
  # 测试不同的邻居数
  for k in a_index:
      kcs = KNN(n_neighbors=k)
      kcs.fit(X_train, y_train)
      y_pred = kcs.predict(X_test)
      accuracy = compute_accuracy(y_test, y_pred)
      accuracies.append(accuracy)
  
  # 转换为Pandas Series
  a_series = pd.Series(accuracies, index=a_index)
  
  # 绘制结果
  plt.plot(a_index, a_series)
  plt.xlabel('Number of Neighbors')
  plt.ylabel('Accuracy')
  plt.xticks(a_index)
  plt.title('KNN Accuracy for Different k Values')
  plt.show()
  
  test_csv = pd.read_csv('data/test_data.csv')
  test_pred = test_csv[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']]
  
  # 检查数据类型并转换为浮点型
  test_pred = test_pred.astype(float)
  
  # 确保数据为NumPy数组
  
  test_pred = test_pred.values
  
  #对test_csv文件的每一项进行预测
  
  predictions = model.predict(test_pred)
  predictions = predictions.astype(int)
  predictions = pd.DataFrame(predictions, columns=['label'])
  predictions.to_csv('task1_test_prediction.csv', index=False)
  predictions.head().style.background_gradient(sns.color_palette("YlOrBr", as_cmap=True))
  
  test_csv2 = pd.read_csv('data/test_data.csv')
  test_pred2 = test_csv[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']]
  
  # 检查数据类型并转换为浮点型
  test_pred2 = test_pred2.astype(float)
  
  # 确保数据为NumPy数组
  
  test_pred2 = test_pred2.values
  
  #对test_csv文件的每一项进行预测
  
  predictions2 = model2.predict(test_pred2)
  predictions2 = predictions2.astype(int)
  predictions2 = pd.DataFrame(predictions2, columns=['label'])
  predictions2.to_csv('task2_test_prediction.csv', index=False)
  iris_train.head().style.background_gradient(sns.color_palette("YlOrBr", as_cmap=True))
  
  # 读取两个CSV文件
  file1 = pd.read_csv('task1_test_prediction.csv')
  file2 = pd.read_csv('task2_test_prediction.csv')
  
  # 比较两个DataFrame并找出不同
  comparison = file1 == file2
  
  # 找出不同的行和列
  differences = comparison[comparison == False]
  
  # 打印出不同的值和位置
  print("Differences found at these locations:")
  print(differences)
  
  # 显示不同的行
  diff_rows = differences.dropna(how='all')
  print("Rows with differences:")
  print(diff_rows)
  
  # 显示不同的列
  diff_cols = differences.dropna(axis=1, how='all')
  print("Columns with differences:")
  print(diff_cols)
\end{lstlisting}

\subsubsection{knn\_mindspore.ipynb}
\begin{lstlisting}[language=Python]
  import mindspore.context as context
  import mindspore.numpy as mnp
  import mindspore.ops as ops
  import pandas as pd
  from mindspore import Tensor
  
  # 设置MindSpore的运行环境
  context.set_context(mode=context.GRAPH_MODE, device_target="CPU")
  
  # 自定义数据加载函数
  def load_data(file_path, has_label=True):
      data = pd.read_csv(file_path)
      if has_label:
          X = data.iloc[:, :-1].values
          y = data.iloc[:, -1].values
          return X, y
      else:
          X = data.values
          return X
  
  # 加载数据
  X_train, y_train = load_data('data/train.csv')
  X_val, y_val = load_data('data/val.csv')
  X_test = load_data('data/test_data.csv', has_label=False)
  
  # 将数据转换为MindSpore张量
  X_train = Tensor(X_train, mnp.float32)
  y_train = Tensor(y_train, mnp.int32)
  X_val = Tensor(X_val, mnp.float32)
  y_val = Tensor(y_val, mnp.int32)
  X_test = Tensor(X_test, mnp.float32)
  
  # 定义KNN算法
  class KNN:
      def __init__(self, k=3):
          self.k = k
      
      def fit(self, X, y):
          self.X_train = X
          self.y_train = y
      
      def predict(self, X):
          distances = self.compute_distances(X)
          return self.predict_labels(distances)
      
      def compute_distances(self, X):
          num_test = X.shape[0]
          num_train = self.X_train.shape[0]
          dists = mnp.zeros((num_test, num_train))
          for i in range(num_test):
              dists[i, :] = mnp.sqrt(mnp.sum((self.X_train - X[i, :])**2, axis=1))
          return dists
      
      def predict_labels(self, dists):
          num_test = dists.shape[0]
          y_pred = mnp.zeros(num_test, dtype=mnp.int32)
          for i in range(num_test):
              closest_y = []
              sorted_indices = ops.Sort(axis=0)(dists[i, :])[1]
              closest_y = self.y_train[sorted_indices[:self.k]]
              y_pred[i] = mnp.bincount(closest_y).argmax()
          return y_pred
  
  # 创建KNN实例并训练
  knn = KNN(k=3)
  knn.fit(X_train, y_train)
  
  # 在验证集上进行预测
  y_val_pred = knn.predict(X_val)
  accuracy_val = mnp.mean((y_val_pred == y_val).astype(mnp.float32))
  print("Validation Accuracy:", accuracy_val.asnumpy())
  
  # 在测试集上进行预测
  y_test_pred = knn.predict(X_test)
  print("Test Predictions:", y_test_pred.asnumpy())
  
  # 将预测结果保存到CSV文件
  df_predictions = pd.DataFrame(y_test_pred.asnumpy(), columns=['label'])
  df_predictions.to_csv('task3_test_prediction.csv', index=False)  
\end{lstlisting}

% 实验二
\newpage
\section{\centering 实验二 Na\"ive Bayes Classification}

\subsection{问题描述}
\subsubsection{概述}
利用朴素贝叶斯算法，对MNIST数据集中的测试集
进行分类。
\subsubsection{任务说明}

\begin{enumerate}
  \item 在课程学习中同学们已经学习了贝叶斯分类理论并掌握了其基本原理，即利用贝叶斯公式
        \[
          p(\omega_j|x) = \frac{p(x|\omega_j)p(\omega_j)}{p(x)}
        \]
        对\(p(\omega_j|x)\)作出预测。由于\(p(x)\)为一固定值，所以一般不在计算过程中求得\(p(x)\)的具体值。在实际运用中，为了方便计算，通常假设数据特征之间相互独立，即
        \[
          p(x|\omega_j) = p(x_1|\omega_j) \cdot p(x_2|\omega_j) \cdots p(x_d|\omega_j), \quad x \in \mathbb{R}^d,
        \]
        这便是著名的朴素贝叶斯算法。

  \item MNIST数据集本身以二进制形式保存，所以首先需要选择合适的编程语言编写读写二进制数据的程序完成对图片、标记信息的初步提取工作。读取了图片信息后，发现每个像素点的值在[0,1]区间内，这是图像压缩后的结果，所以可以先将像素值乘以255再取整，得到每一个点的灰度值。将图像二值化，得到可以用于分类的28×28个特征向量以及对应的标签数据，之后便可以交由贝叶斯分类器进行学习。

  \item 基于MindSpore平台提供的官方模型库，对相同的数据集进行训练，并与自己独立实现的算法对比结果（包括但不限于准确率、算法迭代收敛次数等指标），并分析结果中出现差异的可能原因，给出使用MindSpore的心得和建议。

  \item （加分项）使用MindSpore平台提供的相似任务数据集（例如，其他的分类任务数据集）测试自己独立实现的算法并与MindSpore平台上的官方实现算法进行对比，并进一步分析差异及其成因。
\end{enumerate}

\subsection{实现步骤与流程}
\subsubsection{实验思路}
\begin{enumerate}
  \item 读取数据集的图片和标签信息；
  \item 对图片信息进行预处理，包括归一化、二值化和将图像展开成一维向量；
  \item 实现朴素贝叶斯算法，包括拟合和预测两个步骤；
  \item 使用预训练模型对测试数据进行预测，计算准确率；
  \item 可视化部分模型对测试数据的预测结果。
\end{enumerate}

\subsubsection{数学模型}

朴素贝叶斯学习步骤如下。先计算类先验概率分布：
\begin{equation}
  P(Y = c_k) = \frac{1}{N} \sum_{i=1}^{N} I(\hat{y}_i = c_k), \quad k = 1,2,\cdots,K
\end{equation}

其中$c_k$表示第$k$个类别，$y_i$表示第$i$个样本的类标记。类先验概率分布可以通过极大似然估计得到。

然后计算类条件概率分布：
\begin{equation}
  P(X = x | Y = c_k) = P(X^{(1)} = x^{(1)}, \cdots, X^{(n)} = x^{(n)} | Y = c_k), \quad k = 1,2,\cdots,K
\end{equation}

直接对$P(X = x | Y = c_k)$进行估计不太可行，因为参数量太大。但是朴素贝叶斯的一个最重要的假设就是条件独立性假设，即：
\begin{equation}
  P(X = x | Y = c_k) = P(X^{(1)} = x^{(1)}, \cdots, X^{(n)} = x^{(n)} | Y = c_k) = \prod_{j=1}^{n} P(X^{(j)} = x^{(j)} | Y = c_k)
\end{equation}

有了条件独立性假设之后，便可基于极大似然估计计算类条件概率。

类先验概率分布和类条件概率分布都计算得到之后，基于贝叶斯公式即可以计算类后验概率：
\begin{equation}
  P(Y = c_k | X = x) = \frac{P(X = x | Y = c_k) P(Y = c_k)}{\sum_{k} P(X = x | Y = c_k) P(Y = c_k)}
\end{equation}

代入类条件计算公式，有：
\begin{equation}
  P(Y = c_k | X = x) = \frac{\prod_{j=1}^{n} P(X^{(j)} = x^{(j)} | Y = c_k) P(Y = c_k)}{\sum_{k} \prod_{j=1}^{n} P(X^{(j)} = x^{(j)} | Y = c_k) P(Y = c_k)}
\end{equation}

基于上述公式即可以学习一个朴素贝叶斯模型。给定新的数据样本时，计算其最大后验概率即可：
\begin{equation}
  \hat{y} = \arg \max_{c_k} \frac{\prod_{j=1}^{n} P(X^{(j)} = x^{(j)} | Y = c_k) P(Y = c_k)}{\sum_{k} \prod_{j=1}^{n} P(X^{(j)} = x^{(j)} | Y = c_k) P(Y = c_k)}
\end{equation}

其中，分母对于所有的$\hat{y}$都是一样的，所以上述式可进一步简化为：
\begin{equation}
  \hat{y} = \arg \max_{c_k} \prod_{j=1}^{n} P(X^{(j)} = x^{(j)} | Y = c_k) P(Y = c_k)  \label{naive bayes yhat}
\end{equation}

方程~\ref{naive bayes yhat} 即为朴素贝叶斯算法的预测公式。

\subsubsection{关键难点}
朴素贝叶斯算法的难点在于如何高效地计算类条件概率分布。由于朴素贝叶斯算法的条件独立性假设，
可以将类条件概率分布分解为各个特征的条件概率分布的乘积。这样可以大大减少计算量。

\subsubsection{算法描述}
朴素贝叶斯算法实现的伪代码如算法~\ref{Naive Bayes Classifier} 所示。
\begin{algorithm}[H]
  \caption{朴素贝叶斯分类器}
  \label{Naive Bayes Classifier}
  \begin{algorithmic}[1]
    \State \textbf{初始化:}
    \State 定义类别列表 $classes$
    \State 定义类别先验概率 $class\_priors$
    \State 定义特征概率 $feature\_probs$

    \Procedure{fit}{X, y}
    \State $classes = \text{np.unique}(y)$
    \For{每个类别 $cls$ in $classes$}
    \State $X_{cls} = X[y == cls]$
    \State $P(cls) = \frac{|X_{cls}|}{|X|}$
    \State $P(x_i | cls) = \frac{\sum X_{cls,i} + 1}{|X_{cls}| + 2}$
    \EndFor
    \EndProcedure

    \Procedure{predict}{X}
    \For{每个样本 $x$ in X}
    \For{每个类别 $cls$ in $classes$}
    \State $\log P(cls | x) = \log P(cls) + \sum (\log P(x_i | cls) \times x_i + \log (1 - P(x_i | cls)) \times (1 - x_i))$
    \EndFor
    \State 选择最大后验概率对应的类别
    \EndFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}


\subsection{实验结果与分析}
\subsubsection{手动实现朴素贝叶斯算法}
利用课程所提供的MNIST数据集，我们手动实现了朴素贝叶斯算法，并在测试集上进行了分类。可以看出实验的准确率为\textbf{84.27\%}。

为便于观察手动实现朴素贝叶斯算法模型的性能，我们在部分验证集上进行了测试，测试结果如图~\ref{fig:手动实现Na\"ive Bayes测试结果} 所示。
可以看出在多数图片的预测中，手动实现的朴素贝叶斯算法还是能够将MNIST数据集中的数字进行正确分类的。

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.6]{figures/naive/manual_8427.png}
  \caption{手动实现Na\"ive Bayes测试结果}
  \label{fig:手动实现Na\"ive Bayes测试结果}
\end{figure}


% \begin{figure}[htbp]
%   \centering
%   \caption{手动实现Na\"ive Bayes测试结果}
%   \label{fig:手动实现Na\"ive Bayes测试结果}
%   \begin{minipage}{0.49\linewidth}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/naive/1_1.png}
%     % \caption{Na\"ive Bayes测试一}
%     % \label{Na\"ive Bayes测试一}%文中引用该图片代号
%   \end{minipage}
%   \begin{minipage}{0.49\linewidth}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/naive/1_2.png}
%     % \caption{Na\"ive Bayes测试二}
%     % \label{Na\"ive Bayes测试二}%文中引用该图片代号
%   \end{minipage}
%   %\qquad
%   %让图片换行，

%   \begin{minipage}{0.49\linewidth}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/naive/1_3.png}
%     % \caption{Na\"ive Bayes测试三}
%     % \label{Na\"ive Bayes测试三}%文中引用该图片代号
%   \end{minipage}
%   \begin{minipage}{0.49\linewidth}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/naive/1_4.png}
%     % \caption{Na\"ive Bayes测试四}
%     % \label{Na\"ive Bayes测试四}%文中引用该图片代号
%   \end{minipage}


%   \begin{minipage}{0.49\linewidth}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/naive/1_5.png}
%     % \caption{Na\"ive Bayes测试五}
%     % \label{Na\"ive Bayes测试五}%文中引用该图片代号
%   \end{minipage}
%   \begin{minipage}{0.49\linewidth}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/naive/1_6.png}
%     % \caption{Na\"ive Bayes测试六}
%     % \label{Na\"ive Bayes测试六}%文中引用该图片代号
%   \end{minipage}


%   \begin{minipage}{0.49\linewidth}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/naive/1_7.png}
%     % \caption{Na\"ive Bayes测试七}
%     % \label{Na\"ive Bayes测试七}%文中引用该图片代号
%   \end{minipage}
%   \begin{minipage}{0.49\linewidth}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/naive/1_8.png}
%     % \caption{Na\"ive Bayes测试八}
%     % \label{Na\"ive Bayes测试八}%文中引用该图片代号
%   \end{minipage}


%   \begin{minipage}{0.49\linewidth}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/naive/1_9.png}
%     % \caption{Na\"ive Bayes测试九}
%     % \label{Na\"ive Bayes测试九}%文中引用该图片代号
%   \end{minipage}
%   \begin{minipage}{0.49\linewidth}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/naive/1_10.png}
%     % \caption{Na\"ive Bayes测试十}
%     % \label{Na\"ive Bayes测试十}%文中引用该图片代号
%   \end{minipage}


% \end{figure}

\subsubsection{mindSpore实现朴素贝叶斯算法}
我们使用了mindspore.numpy来代替原有的numpy实现了朴素贝叶斯算法。

使用mindspore实现的朴素贝叶斯算法在MNIST数据集上的分类准确率为\textbf{84.27\%}。准确率和手动实现的朴素贝叶斯算法相同，
说明mindspore实现的朴素贝叶斯算法是正确的。

但是我发现，mindspore.numpy实现要比numpy花费的时间要长，这可能是因为：
\begin{enumerate}
  \item MindSpore 的计算图模式（GRAPH\_MODE）可以优化性能，但需要一些额外的编译时间。相比之下，使用 NumPy 的代码是动态执行的，没有编译阶段。
  \item MindSpore 中的 Tensor 操作在某些情况下可能比 NumPy 更慢，特别是在小规模数据和简单计算时。因为 MindSpore 的设计初衷是用于大规模的深度学习任务，优化了大规模数据的处理，而不是小规模的简单任务。
  \item 代码中有很多从 NumPy 到 mindspore.Tensor 的类型转换，这些操作本身也会带来一些开销。类型转换会导致额外的计算时间和内存开销。
\end{enumerate}





\subsubsection{手动实现算法的评价}

实验结果表明，手动实现的朴素贝叶斯算法在MNIST数据集上的分类准确率为\textbf{84.27\%}。
这个准确率表明多数的MNIST数据集中的数字都能够被正确分类。该结果初步表明手动实现朴素贝叶斯算法的性能是可以接受的。

未来可以采用半朴素贝叶斯算法，适当考虑一部分属性间的相互依赖信息，从而既不需要进行完全联合概率计算，又不至于彻底忽略了比较强的相互依赖关系。
因为朴素贝叶斯党的假设是：假设属性之间相互独立，但实际上属性之间是有关联的，所以半朴素贝叶斯算法是对朴素贝叶斯算法的一种改进。


\subsection{MindSpore 学习使用心得体会}
在这个实验中，我们使用了mindspore.numpy来代替原有的numpy实现了朴素贝叶斯算法。我感觉mindspore的使用还是不如直接用numpy方便，
因为mindspore.mumpy中没有frombuffer这个属性，所以在读取MNIST数据集时，我是使用numpy来读取数据，然后再转换成mindspore.Tensor。
但是这个示例为我深入理解mindspore的操作提供了一个很好的机会。


\subsection{代码附录}
\subsubsection{naive\_bayes.ipynb}
\begin{lstlisting}[language=Python]
  import numpy as np
  import struct
  import gzip
  import matplotlib.pyplot as plt
  def read_images(file_path):
      with gzip.open(file_path, 'rb') as f:
          # 读取文件头信息：魔数和图片数量
          magic, num_images = struct.unpack(">II", f.read(8))
          # 读取图片的行数和列数
          num_rows, num_cols = struct.unpack(">II", f.read(8))
          print(f"Magic number: {magic}, Number of images: {num_images}, Rows: {num_rows}, Columns: {num_cols}")
          # 读取图片数据
          images = np.frombuffer(f.read(), dtype=np.uint8).reshape(num_images, num_rows, num_cols)
          return images
  
  def read_labels(file_path):
      with gzip.open(file_path, 'rb') as f:
          # 读取文件头信息：魔数和标签数量
          magic, num_labels = struct.unpack(">II", f.read(8))
          print(f"Magic number: {magic}, Number of labels: {num_labels}")
          # 读取标签数据
          labels = np.frombuffer(f.read(), dtype=np.uint8)
          return labels
  
  # 设置数据集文件路径
  train_images_path = 'data/train-images-idx3-ubyte.gz'
  train_labels_path = 'data/train-labels-idx1-ubyte.gz'
  test_images_path = 'data/t10k-images-idx3-ubyte.gz'
  test_labels_path = 'data/t10k-labels-idx1-ubyte.gz'
  
  # 读取数据集
  X_train = read_images(train_images_path)
  y_train = read_labels(train_labels_path)
  X_test = read_images(test_images_path)
  y_test = read_labels(test_labels_path)
  
  print(f"Training data shape: {X_train.shape}")
  print(f"Training labels shape: {y_train.shape}")
  print(f"Test data shape: {X_test.shape}")
  print(f"Test labels shape: {y_test.shape}")
  # 将像素值归一化到[0, 1]范围
  X_train = X_train / 255.0
  X_test = X_test / 255.0
  
  # 将图像二值化
  X_train = (X_train > 0.5).astype(int)
  X_test = (X_test > 0.5).astype(int)
  
  # 将图像展开成一维向量
  X_train = X_train.reshape(X_train.shape[0], -1)
  X_test = X_test.reshape(X_test.shape[0], -1)
  class OptimizedMultinomialNaiveBayes:
      def __init__(self, alpha=1.0):
          self.alpha = alpha  # 平滑参数
          self.classes = None
          self.class_count = None
          self.feature_count = None
          self.class_log_prior = None
          self.feature_log_prob = None
  
      def fit(self, X, y):
          self.classes = np.unique(y)
          n_classes = len(self.classes)
          n_features = X.shape[1]
          self.class_count = np.zeros(n_classes)
          self.feature_count = np.zeros((n_classes, n_features))
  
          for idx, c in enumerate(self.classes):
              X_c = X[y == c]
              self.class_count[idx] = X_c.shape[0]
              self.feature_count[idx, :] = np.sum(X_c, axis=0)
  
          self.class_log_prior = np.log(self.class_count / np.sum(self.class_count))
          self.feature_log_prob = np.log((self.feature_count + self.alpha) / (self.class_count[:, None] + self.alpha * n_features))
  
      def predict(self, X):
          log_likelihood = X @ self.feature_log_prob.T
          log_posterior = log_likelihood + self.class_log_prior
          return self.classes[np.argmax(log_posterior, axis=1)]
  # 实例化并训练优化后的多项式朴素贝叶斯分类器
  omnb = OptimizedMultinomialNaiveBayes(alpha=1.0)
  omnb.fit(X_train, y_train)
  
  # 在测试集上进行预测
  predictions = omnb.predict(X_test)
  
  # 计算准确率
  accuracy = np.mean(predictions == y_test)
  print(f'Accuracy: {accuracy}')
  # 可视化前几个预测结果
  for i in range(10):
      plt.imshow(X_test[i].reshape(28, 28), cmap='gray')
      plt.title(f'Predicted: {predictions[i]}, Actual: {y_test[i]}')
      plt.show()
  
\end{lstlisting}
\subsubsection{naive\_bayes\_mindspore.ipynb}
\begin{lstlisting}[language=Python]
  import struct
  import gzip
  import matplotlib.pyplot as plt
  import numpy as np  # 用于从缓冲区读取数据
  import mindspore.context as context
  import mindspore.numpy as mnp
  import mindspore.ops as ops
  from mindspore import Tensor
  
  # 设置MindSpore的运行环境
  context.set_context(mode=context.GRAPH_MODE, device_target="CPU")
  
  def read_images(file_path):
      with gzip.open(file_path, 'rb') as f:
          # 读取文件头信息：魔数和图片数量
          magic, num_images = struct.unpack(">II", f.read(8))
          # 读取图片的行数和列数
          num_rows, num_cols = struct.unpack(">II", f.read(8))
          print(f"Magic number: {magic}, Number of images: {num_images}, Rows: {num_rows}, Columns: {num_cols}")
          # 读取图片数据
          buffer = f.read()
          images = np.frombuffer(buffer, dtype=np.uint8).reshape(num_images, num_rows, num_cols)
          return Tensor(images, dtype=mnp.float32)
  
  def read_labels(file_path):
      with gzip.open(file_path, 'rb') as f:
          # 读取文件头信息：魔数和标签数量
          magic, num_labels = struct.unpack(">II", f.read(8))
          print(f"Magic number: {magic}, Number of labels: {num_labels}")
          # 读取标签数据
          buffer = f.read()
          labels = np.frombuffer(buffer, dtype=np.uint8)
          return Tensor(labels, dtype=mnp.int32)
  
  # 设置数据集文件路径
  train_images_path = 'data/train-images-idx3-ubyte.gz'
  train_labels_path = 'data/train-labels-idx1-ubyte.gz'
  test_images_path = 'data/t10k-images-idx3-ubyte.gz'
  test_labels_path = 'data/t10k-labels-idx1-ubyte.gz'
  
  # 读取数据集
  X_train = read_images(train_images_path)
  y_train = read_labels(train_labels_path)
  X_test = read_images(test_images_path)
  y_test = read_labels(test_labels_path)
  
  print(f"Training data shape: {X_train.shape}")
  print(f"Training labels shape: {y_train.shape}")
  print(f"Test data shape: {X_test.shape}")
  print(f"Test labels shape: {y_test.shape}")
  
  # 将数据转换为mindspore.numpy数组
  X_train = mnp.array(X_train) / 255.0
  X_test = mnp.array(X_test) / 255.0
  y_train = mnp.array(y_train)
  y_test = mnp.array(y_test)
  
  # 将图像二值化
  X_train = (X_train > 0.5).astype(mnp.float32)
  X_test = (X_test > 0.5).astype(mnp.float32)
  
  # 将图像展开成一维向量
  X_train = X_train.reshape(X_train.shape[0], -1)
  X_test = X_test.reshape(X_test.shape[0], -1)
  
  class NaiveBayes:
      def __init__(self):
          self.classes = None
          self.class_priors = {}
          self.feature_probs = {}
  
      def fit(self, X, y):
          self.classes = mnp.unique(y)
          n_samples, n_features = X.shape
  
          for cls in self.classes:
              indices = mnp.where(y == cls, mnp.ones_like(y), mnp.zeros_like(y)).astype(bool)
              X_cls = X[indices]
              self.class_priors[int(cls.asnumpy())] = X_cls.shape[0] / n_samples
              self.feature_probs[int(cls.asnumpy())] = (X_cls.sum(axis=0) + 1) / (X_cls.shape[0] + 2)
  
      def predict(self, X):
          posteriors = []
          for x in X:
              posterior_probs = {}
              for cls in self.classes:
                  cls_int = int(cls.asnumpy())
                  prior = mnp.log(Tensor(self.class_priors[cls_int], dtype=mnp.float32))
                  conditional = mnp.sum(mnp.log(Tensor(self.feature_probs[cls_int], dtype=mnp.float32)) * x + mnp.log(1 - Tensor(self.feature_probs[cls_int], dtype=mnp.float32)) * (1 - x))
                  posterior_probs[cls_int] = prior + conditional
              posteriors.append(max(posterior_probs, key=posterior_probs.get))
          return mnp.array(posteriors)
  
  # 实例化朴素贝叶斯分类器
  nb = NaiveBayes()
  
  # 对数据进行训练
  nb.fit(X_train, y_train)
  
  # 对测试数据进行预测
  y_pred = nb.predict(X_test)
  accuracy = mnp.mean((y_pred == y_test).astype(mnp.float32))
  print(f"Accuracy: {accuracy.asnumpy():.4f}")
  
  # 可视化部分预测结果
  import random
  
  # 随机选择10个测试样本
  indices = random.sample(range(X_test.shape[0]), 10)
  images = X_test[indices].reshape(-1, 28, 28)
  true_labels = y_test[indices]
  pred_labels = y_pred[indices]
  
  plt.figure(figsize=(10, 10))
  for i in range(10):
      plt.subplot(5, 5, i + 1)
      plt.imshow(images[i].asnumpy(), cmap='gray')
      plt.title(f"True: {true_labels[i].asnumpy()}\nPred: {pred_labels[i].asnumpy()}")
      plt.axis('off')
  plt.tight_layout()
  plt.show()  
  
\end{lstlisting}

% 实验三
\newpage
\section{\centering 实验三 Neural Network Image Classification}

\subsection{问题描述}
\subsection{概述}
利用神经网络算法，对CIFAR数据集中的测试集进
行分类。
\subsection{任务说明}
\begin{enumerate}%[label=\(\Box\)]
  \item 基于神经网络模型及BP算法，根据训练集中的数据对你设计的神经网络模型进行训练，随后对给定的打乱的测试集中的数据进行分类。

  \item 基于MindSpore平台提供的官方模型库，对相同的数据集进行训练，并与自己独立实现的算法对比结果（包括但不限于准确率、算法迭代收敛次数等指标），并分析结果中出现差异的可能原因。

  \item （加分项）使用MindSpore平台提供的相似任务数据集（例如，其他的分类任务数据集）测试自己独立实现的算法并与MindSpore平台上的官方实现算法进行对比，并进一步分析差异及其成因。
\end{enumerate}

\subsection{实现步骤与流程}
\subsubsection{实验环境}
实验环境见表~\ref{tab:indicators}。

\begin{table}[!t]
  \caption{Experiment Environment}
  \label{tab:indicators}
  \centering
  \begin{tabular}{m{5cm}<{\centering}m{5cm}<{\centering}m{4cm}<{\centering}}
    \toprule
    \textbf{Items}   & \textbf{Version}     \\[\medskipamount]
    \midrule
    CPU              & Intel Core i5-1135G7 \\[\medskipamount]
    RAM              & 16 GB                \\[\medskipamount]
    Python           & 3.11.5               \\[\medskipamount]
    % scikit-learn     & 1.3.2                \\[\medskipamount]
    Operating system & Windows11            \\[\medskipamount]
    \bottomrule
  \end{tabular}
\end{table}

\subsubsection{实验思路}
\begin{enumerate}
  \item \textbf{数据加载与预处理}:
        \begin{enumerate}
          \item 从CIFAR-10数据集中加载图像数据和标签。
          \item 将图像数据从原始格式转换为适用于神经网络的格式（32 $\times$ 32 $\times$ 3 $\rightarrow$ 3072 $\times$ 1）。
          \item 将图像数据进行归一化处理，将像素值缩放到[0, 1]范围内。
        \end{enumerate}

  \item \textbf{数据划分}:
        \begin{enumerate}
          \item 将加载的训练数据划分为训练集和验证集，以便在训练过程中进行模型评估。
        \end{enumerate}

  \item \textbf{定义全连接神经网络结构}:
        \begin{enumerate}
          \item 输入层：包含3072个神经元，对应每张图像的3072个像素值。
          \item 隐藏层：包含100个神经元，使用ReLU激活函数。
          \item 输出层：包含10个神经元，对应10个类别，使用Softmax激活函数。
        \end{enumerate}

  \item \textbf{定义前向传播与反向传播}:
        \begin{enumerate}
          \item 前向传播：计算输入数据通过网络后的输出。
          \item 反向传播：根据预测结果和实际标签计算损失，并更新网络权重。
        \end{enumerate}

  \item \textbf{模型训练}:
        \begin{enumerate}
          \item 使用小批量梯度下降优化网络权重。
          \item 在每个epoch结束后，计算并记录训练损失、验证损失和验证准确率。
        \end{enumerate}

  \item \textbf{模型评估}:
        \begin{enumerate}
          \item 在测试集上评估模型性能，计算并输出测试集准确率。
          \item 绘制训练过程中损失和准确率的变化曲线。
        \end{enumerate}

  \item \textbf{混淆矩阵}:
        \begin{enumerate}
          \item 计算混淆矩阵，分析分类结果的具体表现。
          \item 绘制混淆矩阵，直观展示模型在各个类别上的分类效果。
        \end{enumerate}
  \item \textbf{预测结果展示}
        \begin{enumerate}
          \item 展示模型在测试集部分图片上的分类结果。
        \end{enumerate}
\end{enumerate}
本实验的评估指标和超参数如表~\ref{tab:training-params} 所示。
\begin{table}[htbp]
  \centering
  \caption{实验评估指标和超参数}
  \label{tab:training-params}
  \begin{tabular}{m{3cm}<{\centering}m{3cm}<{\centering}}
    \toprule
    \textbf{参数}   & \textbf{值}    \\[\medskipamount]
    \midrule
    learning rate & 0.01          \\[\medskipamount]
    epochs        & 100           \\[\medskipamount]
    % optimizer     & SGD           \\[\medskipamount]
    loss function & Cross Entropy \\[\medskipamount]
    performance   & accuracy      \\[\medskipamount]
    batch size    & 64            \\[\medskipamount]
    num hiddens   & 128           \\[\medskipamount]
    \bottomrule
  \end{tabular}
\end{table}


\subsubsection{数学模型}
BP神经网络的数学模型如图~\ref{fig:fcnn_structure} 所示。
\tikzset{%
  neuron missing/.style={
      draw=none,
      scale=4,
      text height=0.333cm,
      execute at begin node=\color{black}$\vdots$
    },
}
\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]

    \foreach \m/\l [count=\y] in {1,2,3}
      {
        \node [circle,fill=green!50,minimum size=1cm] (input-\m) at (0,2.5-\y) {};
      }
    \foreach \m/\l [count=\y] in {4}
      {
        \node [circle,fill=green!50,minimum size=1cm ] (input-\m) at (0,-2.5) {};
      }

    \node [neuron missing]  at (0,-1.5) {};


    \foreach \m [count=\y] in {1}
    \node [circle,fill=red!50,minimum size=1cm ] (hidden-\m) at (2,0.75) {};

    \foreach \m [count=\y] in {2}
    \node [circle,fill=red!50,minimum size=1cm ] (hidden-\m) at (2,-1.85) {};

    \node [neuron missing]  at (2,-0.3) {};


    \foreach \m [count=\y] in {1}
    \node [circle,fill=blue!50,minimum size=1cm ] (output-\m) at (4,1.5-\y) {};

    \foreach \m [count=\y] in {2}
    \node [circle,fill=blue!50,minimum size=1cm ] (output-\m) at (4,-0.5-\y) {};

    \node [neuron missing]  at (4,-0.4) {};

    \foreach \l [count=\i] in {1,2,3, 3072}
    \draw [<-] (input-\i) -- ++(-1,0)
    node [above, midway] {$I_{\l}$};

    \foreach \l [count=\i] in {1,128}
    \node [above] at (hidden-\i.north) {$H_{\l}$};

    \foreach \l [count=\i] in {1,10}
    \draw [->] (output-\i) -- ++(1,0)
    node [above, midway] {$O_{ \l}$};

    \foreach \i in {1,...,4}
    \foreach \j in {1,...,2}
    \draw [->] (input-\i) -- (hidden-\j);

    \foreach \i in {1,...,2}
    \foreach \j in {1,...,2}
    \draw [->] (hidden-\i) -- (output-\j);

    %\foreach \l [count=\x from 0] in {Input, Hidden, Output}
    % \node [align=center, above] at (\x*2,2) {\l \\ layer};

  \end{tikzpicture}
  \caption{全连接神经网络结构示意图}
  \label{fig:fcnn_structure}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]

    % Input layer
    \foreach \m/\l [count=\y] in {1,2,3}
      {
        \node [circle,fill=green!50,minimum size=1cm] (input-\m) at (0,2.5-\y) {};
      }
    \foreach \m/\l [count=\y] in {4}
      {
        \node [circle,fill=green!50,minimum size=1cm ] (input-\m) at (0,-2.5) {};
      }
    \node [neuron missing]  at (0,-1.5) {};

    % Hidden layer 1
    \foreach \m/\l [count=\y] in {1,2,3}
      {
        \node [circle,fill=red!50,minimum size=1cm ] (hidden1-\m) at (2,2.5-\y) {};
      }
    \foreach \m/\l [count=\y] in {4}
      {
        \node [circle,fill=red!50,minimum size=1cm ] (hidden1-\m) at (2,-2.5) {};
      }
    \node [neuron missing]  at (2,-1.5) {};

    % Hidden layer 2
    \foreach \m/\l [count=\y] in {1,2,3}
      {
        \node [circle,fill=yellow!50,minimum size=1cm ] (hidden2-\m) at (4,2.5-\y) {};
      }
    \foreach \m/\l [count=\y] in {4}
      {
        \node [circle,fill=yellow!50,minimum size=1cm ] (hidden2-\m) at (4,-2.5) {};
      }
    \node [neuron missing]  at (4,-1.5) {};

    % Output layer
    \foreach \m/\l [count=\y] in {1,2,3}
      {
        \node [circle,fill=blue!50,minimum size=1cm ] (output-\m) at (6,2.5-\y) {};
      }
    \foreach \m/\l [count=\y] in {4}
      {
        \node [circle,fill=blue!50,minimum size=1cm ] (output-\m) at (6,-2.5) {};
      }
    \node [neuron missing]  at (6,-1.5) {};

    % Connect input to hidden layer 1
    \foreach \i in {1,...,4}
    \foreach \j in {1,...,4}
    \draw [->] (input-\i) -- (hidden1-\j);

    % Connect hidden layer 1 to hidden layer 2
    \foreach \i in {1,...,4}
    \foreach \j in {1,...,4}
    \draw [->] (hidden1-\i) -- (hidden2-\j);

    % Connect hidden layer 2 to output
    \foreach \i in {1,...,4}
    \foreach \j in {1,...,4}
    \draw [->] (hidden2-\i) -- (output-\j);

    % Labels for input layer
    \foreach \l [count=\i] in {1,2,3, 3072}
    \draw [<-] (input-\i) -- ++(-1,0)
    node [above, midway] {$I_{\l}$};

    % Labels for hidden layers
    \node [above] at (hidden1-1.north) {$H1_{1}$};
    \node [above] at (hidden1-4.north) {$H1_{512}$};
    \node [above] at (hidden2-1.north) {$H2_{1}$};
    \node [above] at (hidden2-4.north) {$H2_{512}$};

    % Labels for output layer
    \node [above] at (output-1.north) {$O_{1}$};
    \node [above] at (output-4.north) {$O_{10}$};

  \end{tikzpicture}
  \caption{基于 MindSpore 平台实现的多隐藏层神经网络结构示意图}
  \label{fig:multilayer_nn_structure}
\end{figure}

\subsubsection{关键难点}
在训练过程中实时评估模型的性能，并根据评估结果调整模型的超参数，如学习率、批次大小、隐藏层神经元数量等。
需要平衡模型的复杂度和泛化能力，避免过拟合或欠拟合。

\subsubsection{算法描述}
使用BP神经网络训练算法，如算法~\ref{FullyConnectedNN Training Algorithm} 所示。
% \begin{algorithm}
%   \caption{BP神经网络训练算法}
%   \label{BP Neural Network Training Algorithm}
%   \begin{algorithmic}[1]
%     \State 初始化网络的权重$\mathbf{W}$和偏置$\mathbf{b}$为随机值
%     \Procedure{训练}{X, y, x\_val, y\_val, batch\_size, learning\_rate, epochs}
%       \For{每个epoch}
%         \For{每个mini-batch}
%           \State \textbf{前向传播}:
%           \State \quad 设输入$\mathbf{x}$
%           \State \quad 计算每一层的输出和激活值:
%           \State \quad \quad $\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}$
%           \State \quad \quad $\mathbf{a}^{(l)} = \sigma(\mathbf{z}^{(l)})$
%           \State \quad 其中，$\sigma$为激活函数
%           \State \textbf{计算损失}:
%           \State \quad 使用损失函数计算输出层的损失$L$
%           \State \textbf{反向传播}:
%           \State \quad 计算输出层的误差:
%           \State \quad \quad $\delta^{(L)} = \mathbf{a}^{(L)} - \mathbf{y}$
%           \State \quad 计算隐藏层的误差:
%           \State \quad \quad $\delta^{(l)} = (\mathbf{W}^{(l+1)})^T \delta^{(l+1)} \odot \sigma'(\mathbf{z}^{(l)})$
%           \State \quad 计算梯度:
%           \State \quad \quad $\nabla_{\mathbf{W}^{(l)}} = \delta^{(l)} (\mathbf{a}^{(l-1)})^T$
%           \State \quad \quad $\nabla_{\mathbf{b}^{(l)}} = \delta^{(l)}$
%           \State \textbf{更新权重和偏置}:
%           \State \quad $\mathbf{W}^{(l)} = \mathbf{W}^{(l)} - \eta \nabla_{\mathbf{W}^{(l)}}$
%           \State \quad $\mathbf{b}^{(l)} = \mathbf{b}^{(l)} - \eta \nabla_{\mathbf{b}^{(l)}}$
%         \EndFor
%         \State 计算训练集和验证集的准确率
%       \EndFor
%     \EndProcedure
%   \end{algorithmic}
% \end{algorithm}

\begin{algorithm}
  \caption{全连接神经网络训练算法}
  \label{FullyConnectedNN Training Algorithm}
  \begin{algorithmic}[1]
    \State 初始化网络的权重$\mathbf{W1}, \mathbf{W2}$和偏置$\mathbf{b1}, \mathbf{b2}$为随机值
    \Procedure{训练}{X\_train, Y\_train, X\_val, Y\_val, batch\_size, learning\_rate, epochs}
    \For{每个epoch}
    \For{每个mini-batch}
    \State \textbf{前向传播}:
    \State \quad 设输入$\mathbf{X}$
    \State \quad 计算隐藏层的输出和激活值:
    \State \quad \quad $\mathbf{Z1} = \mathbf{X} \mathbf{W1} + \mathbf{b1}$
    \State \quad \quad $\mathbf{A1} = \max(0, \mathbf{Z1})$ \Comment{ReLU激活函数}
    \State \quad 计算输出层的输出和激活值:
    \State \quad \quad $\mathbf{Z2} = \mathbf{A1} \mathbf{W2} + \mathbf{b2}$
    \State \quad \quad $\mathbf{A2} = \text{softmax}(\mathbf{Z2})$
    \State \textbf{计算损失}:
    \State \quad 使用交叉熵损失函数计算损失$L$
    \State \textbf{反向传播}:
    \State \quad 计算输出层的误差$\delta^{(2)}$
    \State \quad 计算隐藏层的误差$\delta^{(1)}$
    \State \quad 计算梯度:
    \State \quad \quad $\nabla_{\mathbf{W2}} = \mathbf{A1}^T \delta^{(2)}$
    \State \quad \quad $\nabla_{\mathbf{b2}} = \sum \delta^{(2)}$
    \State \quad \quad $\nabla_{\mathbf{W1}} = \mathbf{X}^T \delta^{(1)}$
    \State \quad \quad $\nabla_{\mathbf{b1}} = \sum \delta^{(1)}$
    \State \textbf{更新权重和偏置}:
    \State \quad 使用学习率$\eta$更新$\mathbf{W1}, \mathbf{b1}, \mathbf{W2}, \mathbf{b2}$
    \EndFor
    \State 计算训练集和验证集的损失和准确率
    \EndFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{实验结果与分析}

\subsubsection{实验结果展示}

\begin{enumerate}
  \item \textbf{准确率曲线} 本实验训练集和验证集上的准确率随epoch的变化曲线如图~\ref{fig:accuracy_curve} 所示。
        从图中可以看出，随着epoch的增加，验证集的准确率逐渐提高，最终收敛到\textbf{50\%}。
  \item \textbf{损失曲线} 本实验验证集上的交叉熵损失随epoch的变化曲线如图~\ref{fig:loss_curve} 所示。
        从图中可以看出，随着epoch的增加，训练接和验证集的交叉熵损失逐渐降低，最终收敛到一个稳定值。
        其中训练集交叉熵损失稳定在\textbf{1.0}左右，验证集交叉熵损失稳定在\textbf{1.4}左右。
  \item \textbf{混淆矩阵} 本实验的混淆矩阵如图~\ref{fig:confusion_matrix} 所示。
        从图中可以看出，模型在CIFAR-10数据集上的分类效果较好，大部分类别的分类准确率较高。
  \item \textbf{预测结果} 本实验的预测结果如图~\ref{fig:random_result} 所示。
        从图中可以看出，模型在测试集上的分类效果较好，大部分图片的分类结果正确。
\end{enumerate}

\subsubsection{进一步探究}
虽然上述基于全连接神经网络的分类器在CIFAR-10数据集上取得了一定的分类效果，但是其准确率仍然较低，仅为50\%左右。
结合机器学习课程所学知识，可以尝试以下方法进一步提高分类器的性能：

\begin{itemize}
  \item 尝试使用更复杂的神经网络结构，如CNN、RNN、Transformers等
  \item 尝试使用更高级的优化算法，如Adam、RMSprop等
  \item 尝试使用更复杂的数据增强技术，如旋转、平移、缩放等
  \item 尝试使用更复杂的模型评估指标，如F1-score、ROC曲线等
  \item 尝试使用更复杂的模型融合技术，如集成学习、模型融合等
  \item 尝试使用更复杂的超参数调优技术，如网格搜索、贝叶斯优化等
  \item 尝试使用更复杂的模型解释技术，如LIME、SHAP等
\end{itemize}

\subsubsection{手动实现算法的评价}
本实验实现了单隐藏层前馈神经网络对CIFAR-10数据集进行分类，取得了50\%左右的准确率。
与mindspore平台相比，手动实现的算法在准确率和收敛速度上均有所不足，可能的原因如下：
\begin{enumerate}
  \item \textbf{模型复杂度}：手动实现的算法只使用了单隐藏层的前馈神经网络，模型复杂度较低，难以捕捉数据集的复杂特征。
  \item \textbf{优化算法}：手动实现的算法使用了简单的小批量梯度下降优化算法，收敛速度较慢，难以达到较高的准确率。
        而mindspore中泽用的是SGD、Adam等高级的优化算法。
  \item \textbf{超参数调优}：手动实现的算法中的超参数（学习率、批次大小、隐藏层神经元数量等）未经过充分调优，可能导致模型性能不佳。
\end{enumerate}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.75]{figures/nn/accuracy_curve.png}
  \caption{Accuracy随epoch的变化曲线}
  \label{fig:accuracy_curve}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.75]{figures/nn/loss_curve.png}
  \caption{Loss随epoch的变化曲线}
  \label{fig:loss_curve}
\end{figure}


\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.5]{figures/nn/confusion_matrix.png}
  \caption{Confusion Matrix on CIFAR-10}
  \label{fig:confusion_matrix}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.45]{figures/nn/random_result.png}
  \caption{随机测试数据展示分类效果}
  \label{fig:random_result}
\end{figure}
\subsubsection{手动实现BP神经网络算法和基于Mindspore实现神经网络算法的对比}
\begin{enumerate}
  \item \textbf{准确率对比}：基于Mindspore实现的神经网络在CIFAR-10数据集上的准确率约为\textbf{53\%}，
        高于手动实现的算法的准确率\textbf{50\%}。
  \item \textbf{收敛速度对比}：基于Mindspore实现的神经网络在CIFAR-10数据集上的收敛速度较快，
        仅需\textbf{10}个epoch即可收敛，而手动实现的算法需要\textbf{100}个epoch才能收敛。
  \item \textbf{模型复杂度对比}：基于Mindspore实现的神经网络使用了多隐藏层的神经网络结构，模型复杂度更高，
        能够更好地捕捉数据集的复杂特征。
  \item \textbf{优化算法对比}：基于Mindspore实现的神经网络使用了SGD、Adam等高级的优化算法，收敛速度更快，性能更好。
  \item \textbf{超参数调优对比}：基于Mindspore实现的神经网络中的超参数经过充分调优，模型性能更佳。
\end{enumerate}

%下面需要分析出现上面的差异可能的原因
出现上述差异可能的原因如下：
\begin{enumerate}
  \item \textbf{模型复杂度}：基于Mindspore实现的神经网络使用了多隐藏层的神经网络结构，模型复杂度更高，能够更好地捕捉数据集的复杂特征。
  \item \textbf{优化算法}：基于Mindspore实现的神经网络使用了SGD、Adam等高级的优化算法，收敛速度更快，性能更好。
  \item \textbf{超参数调优}：基于Mindspore实现的神经网络中的超参数经过充分调优，模型性能更佳。
  \item \textbf{数据预处理}：基于Mindspore实现的神经网络使用了更复杂的数据预处理技术，如归一化、标准化、变形等，能够更好地准备数据。
  \item \textbf{自动微分}：基于Mindspore实现的神经网络使用了自动微分技术，能够更方便地计算梯度，简化了模型训练过程。
\end{enumerate}


\subsection{MindSpore 学习使用心得体会}
使用华为MindSpore平台进行深度学习开发与常规的手动实现（如直接使用NumPy等基础库编写神经网络及其训练流程）相比，
带来了若干明显的优势和区别。以下是基于MindSpore平台进行深度学习开发的一些心得体会，
特别是在实现与训练BP神经网络对CIFAR-10数据集进行分类的任务上的体会：
\begin{enumerate}
  \item \textbf{数据加载优势}：MindSpore的dataset模块提供了丰富的数据操作接口，可以方便地从各种来源加载数据。
        使用GeneratorDataset创建自定义数据生成器更为简单，能够快速接入数据流。与手动实现数据加载相比，该方法更高效、更灵活。
  \item \textbf{数据预处理和增强}：通过map操作和MindSpore内置的vision模块，能够在数据流管道中轻松加入图像的预处理和增强步骤，
        如归一化、标准化、变形等。这些操作内部进行了优化，执行速度快，并且调用接口简单明了，易于理解和操作。
  \item \textbf{训练和BP计算过程区别}：在MindSpore中使用nn.Cell来定义网络模型更为简洁，在construct方法中定义数据通过网络的流动。
        这使得模型的构造直观而易于管理，与手动实现（如使用NumPy自行编写前向传播和反向传播）相比，大大降低了开发难度。
  \item \textbf{自动微分}：MindSpore的自动微分机制（通过value\_and\_grad函数）使得计算损失函数对于参数的梯度变得异常简单。
        开发者无需手动推导和编程实现参数的梯度计算，框架会自动完成，这一点是传统手动实现所不能比拟的。
  \item \textbf{优化器和参数更新}：在MindSpore中，优化器已被封装成了内置的类，如nn.SGD，并且可以直接与模型参数关联起来，
        调用优化器的语法简单，使得在训练循环中参数更新的实现非常高效并且易于理解。

\end{enumerate}

\textbf{总结}：使用基于MindSpore的深度学习开发相比于传统的手动实现方式，
在数据加载、预处理、模型构建和训练等多个方面都具有明显的优势。一方面，
MindSpore的丰富数据接口和功能强大的数据预处理功能简化了数据准备工作。另一方面，
通过自动微分及优化器提供的高层封装，极大地减少了梯度计算和参数更新的编程工作量，提升了开发效率。
更不用说，MindSpore平台针对华为Ascend处理器的性能优化，对于训练效率可能带来额外的提升。



\subsection{代码附录}

\subsubsection{cifar-10-scratch.ipynb}
\begin{lstlisting}[language=Python]
  import pickle
  import numpy as np
  import os
  import matplotlib.pyplot as plt
  
  # 加载 CIFAR-10 批次数据
  def load_cifar10_batch(filename):
      with open(filename, 'rb') as f:
          datadict = pickle.load(f, encoding='bytes')
          X = datadict[b'data']
          Y = datadict[b'labels']
          X = X.reshape(10000, 3, 32, 32).astype("float")
          Y = np.array(Y)
          return X, Y
  
  # 加载所有 CIFAR-10 数据
  def load_cifar10(ROOT):
      xs = []
      ys = []
      for b in range(1, 6):
          f = os.path.join(ROOT, 'data_batch_%d' % (b,))
          X, Y = load_cifar10_batch(f)
          xs.append(X)
          ys.append(Y)
      Xtr = np.concatenate(xs)
      Ytr = np.concatenate(ys)
      del X, Y
      Xte, Yte = load_cifar10_batch(os.path.join(ROOT, 'test_batch'))
      return Xtr, Ytr, Xte, Yte
  
  ROOT = './cifar-10-batches-py'
  X_train, y_train, X_test, y_test = load_cifar10(ROOT)
  
  # 全连接神经网络类
  class FullyConnectedNN:
      def __init__(self, input_size, hidden_size, output_size):
          self.W1 = np.random.randn(input_size, hidden_size) * 0.01
          self.b1 = np.zeros((1, hidden_size))
          self.W2 = np.random.randn(hidden_size, output_size) * 0.01
          self.b2 = np.zeros((1, output_size))
  
      def relu(self, Z):
          return np.maximum(0, Z)
  
      def softmax(self, Z):
          expZ = np.exp(Z - np.max(Z))
          return expZ / expZ.sum(axis=1, keepdims=True)
  
      def forward(self, X):
          self.Z1 = np.dot(X, self.W1) + self.b1
          self.A1 = self.relu(self.Z1)
          self.Z2 = np.dot(self.A1, self.W2) + self.b2
          self.A2 = self.softmax(self.Z2)
          return self.A2
  
      def compute_loss(self, Y, Y_hat):
          m = Y.shape[0]
          log_likelihood = -np.log(Y_hat[range(m), Y])
          loss = np.sum(log_likelihood) / m
          return loss
  
      def backward(self, X, Y, Y_hat):
          m = X.shape[0]
          dZ2 = Y_hat
          dZ2[range(m), Y] -= 1
          dZ2 /= m
  
          dW2 = np.dot(self.A1.T, dZ2)
          db2 = np.sum(dZ2, axis=0, keepdims=True)
  
          dA1 = np.dot(dZ2, self.W2.T)
          dZ1 = dA1 * (self.Z1 > 0)
  
          dW1 = np.dot(X.T, dZ1)
          db1 = np.sum(dZ1, axis=0, keepdims=True)
  
          self.W1 -= self.learning_rate * dW1
          self.b1 -= self.learning_rate * db1
          self.W2 -= self.learning_rate * dW2
          self.b2 -= self.learning_rate * db2
  
      def compute_accuracy(self, X, Y):
          Y_hat = self.forward(X)
          predictions = np.argmax(Y_hat, axis=1)
          accuracy = np.mean(predictions == Y)
          return accuracy
  
      def train(self, X_train, Y_train, X_val, Y_val, epochs=300, learning_rate=0.01):
          self.learning_rate = learning_rate
          train_losses = []
          val_losses = []
          val_accuracies = []
  
          for epoch in range(epochs):
              # 打乱训练数据
              indices = np.arange(X_train.shape[0])
              np.random.shuffle(indices)
              X_train = X_train[indices]
              Y_train = Y_train[indices]
  
              # 小批量梯度下降
              for start_idx in range(0, X_train.shape[0], batch_size):
                  end_idx = min(start_idx + batch_size, X_train.shape[0])
                  X_batch = X_train[start_idx:end_idx]
                  Y_batch = Y_train[start_idx:end_idx]
  
                  # 前向传播
                  Y_hat_train = self.forward(X_batch)
                  train_loss = self.compute_loss(Y_batch, Y_hat_train)
                  
                  # 反向传播
                  self.backward(X_batch, Y_batch, Y_hat_train)
  
              # 每个 epoch 结束后计算验证集上的损失和准确率
              Y_hat_val = self.forward(X_val)
              val_loss = self.compute_loss(Y_val, Y_hat_val)
              val_accuracy = self.compute_accuracy(X_val, Y_val)
              
              # 存储损失和准确率
              train_losses.append(train_loss)
              val_losses.append(val_loss)
              val_accuracies.append(val_accuracy)
              
              # 打印每个 epoch 的损失和准确率
              print(f'Epoch [{epoch + 1}], train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_accuracy:.4f}')
          
          return train_losses, val_losses, val_accuracies
  
  # 预处理数据
  X_train = X_train.reshape(X_train.shape[0], -1) / 255.0
  X_test = X_test.reshape(X_test.shape[0], -1) / 255.0
  
  # 将训练数据分成训练集和验证集
  split_index = int(0.8 * X_train.shape[0])
  X_val, y_val = X_train[split_index:], y_train[split_index:]
  X_train, y_train = X_train[:split_index], y_train[:split_index]
  
  # 超参数
  input_size = 3072  # 32*32*3
  hidden_size = 100
  output_size = 10
  learning_rate = 0.01
  epochs = 100
  batch_size = 64
  
  # 初始化并训练模型
  model = FullyConnectedNN(input_size, hidden_size, output_size)
  train_losses, val_losses, val_accuracies = model.train(X_train, y_train, X_val, y_val, epochs, learning_rate)
  
  # 在测试集上进行预测
  y_pred = np.argmax(model.forward(X_test), axis=1)
  accuracy = np.mean(y_pred == y_test)
  print(f'测试集准确率: {accuracy:.4f}')
  
  # 绘制损失曲线
  epochs_range = range(epochs)
  plt.figure(figsize=(8, 4))
  plt.plot(epochs_range, train_losses, label='训练损失')
  plt.plot(epochs_range, val_losses, label='验证损失')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend(loc='upper right')
  plt.title('损失曲线')
  plt.show()
  
  # 绘制准确率曲线
  plt.figure(figsize=(8, 4))
  plt.plot(epochs_range, val_accuracies, label='验证准确率')
  plt.xlabel('Epochs')
  plt.ylabel('Accuracy')
  plt.legend(loc='lower right')
  plt.title('准确率曲线')
  plt.show()
  
  
  # 计算混淆矩阵
  def compute_confusion_matrix(y_true, y_pred, num_classes):
      cm = np.zeros((num_classes, num_classes), dtype=int)
      for i in range(len(y_true)):
          cm[y_true[i], y_pred[i]] += 1
      return cm
  
  # 绘制混淆矩阵
  def plot_confusion_matrix(cm, classes, title='混淆矩阵', cmap=plt.cm.Blues):
      plt.figure(figsize=(16, 12))
      plt.imshow(cm, interpolation='nearest', cmap=cmap)
      plt.title(title)
      plt.colorbar()
      tick_marks = np.arange(len(classes))
      plt.xticks(tick_marks, classes, rotation=45)
      plt.yticks(tick_marks, classes)
  
      fmt = 'd'
      thresh = cm.max() / 2.
      for i, j in np.ndindex(cm.shape):
          plt.text(j, i, format(cm[i, j], fmt),
                   horizontalalignment="center",
                   color="white" if cm[i, j] > thresh else "black")
  
      plt.ylabel('真实标签')
      plt.xlabel('预测标签')
      plt.tight_layout()
      plt.show()
  
  # CIFAR-10 标签
  cifar10_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
  
  # 计算并绘制混淆矩阵
  cm = compute_confusion_matrix(y_test, y_pred, len(cifar10_labels))
  plot_confusion_matrix(cm, classes=cifar10_labels)
  
\end{lstlisting}
\subsubsection{bp\_mindspore.ipynb}
\begin{lstlisting}[language=Python]
  import os
  import pickle
  import numpy as np
  import mindspore
  from mindspore import nn
  from mindspore import dataset as ds
  from mindspore.dataset import vision, transforms
  import matplotlib.pyplot as plt
  
  path = './cifar-10-batches-py'
  
  def load_cifar10_batch(file):
      with open(file, 'rb') as fo:
          dict = pickle.load(fo, encoding='bytes')
      data = dict[b'data']
      labels = dict[b'labels']
      data = data.reshape(len(data), 3, 32, 32).transpose(0, 2, 3, 1)
      return data, labels
  
  def load_cifar10(path):
      x_train = []
      y_train = []
      for i in range(1, 6):
          data, labels = load_cifar10_batch(os.path.join(path, f'data_batch_{i}'))
          x_train.append(data)
          y_train.append(labels)
      x_train = np.concatenate(x_train)
      y_train = np.concatenate(y_train)
      
      x_test, y_test = load_cifar10_batch(os.path.join(path, 'test_batch'))
      
      return (x_train, y_train), (x_test, y_test)
  
  def create_dataset(data, labels, batch_size, shuffle=True):
      def generator():
          for i in range(len(data)):
              yield data[i], labels[i]
  
      dataset = ds.GeneratorDataset(generator, ["image", "label"], shuffle=shuffle)
      image_transforms = [
          vision.Rescale(1.0 / 255.0, 0),
          vision.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)),
          vision.HWC2CHW()
      ]
      label_transform = transforms.TypeCast(mindspore.int32)
      
      dataset = dataset.map(operations=image_transforms, input_columns='image')
      dataset = dataset.map(operations=label_transform, input_columns='label')
      dataset = dataset.batch(batch_size)
      
      return dataset
  
  (train_images, train_labels), (test_images, test_labels) = load_cifar10(path)
  train_dataset = create_dataset(train_images, train_labels, batch_size=64)
  test_dataset = create_dataset(test_images, test_labels, batch_size=64)
  
  class Network(nn.Cell):
      def __init__(self):
          super().__init__()
          self.flatten = nn.Flatten()
          self.dense_relu_sequential = nn.SequentialCell(
              nn.Dense(32*32*3, 512),
              nn.ReLU(),
              nn.Dense(512, 512),
              nn.ReLU(),
              nn.Dense(512, 10)
          )
  
      def construct(self, x):
          x = self.flatten(x)
          logits = self.dense_relu_sequential(x)
          return logits
  
  model = Network()
  epochs = 10
  batch_size = 64
  learning_rate = 1e-2
  loss_fn = nn.CrossEntropyLoss()
  optimizer = nn.SGD(model.trainable_params(), learning_rate=learning_rate)
  
  # Define forward function
  def forward_fn(data, label):
      logits = model(data)
      loss = loss_fn(logits, label)
      return loss, logits
  
  # Get gradient function
  grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)
  
  # Define function of one-step training
  def train_step(data, label):
      (loss, _), grads = grad_fn(data, label)
      optimizer(grads)
      return loss
  
  def train_loop(model, dataset):
      size = dataset.get_dataset_size()
      model.set_train()
      total_loss = 0
      for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):
          loss = train_step(data, label)
          total_loss += loss.asnumpy()
      
      return total_loss / size
  
  def test_loop(model, dataset, loss_fn):
      num_batches = dataset.get_dataset_size()
      model.set_train(False)
      total, test_loss, correct = 0, 0, 0
      all_preds = []
      all_labels = []
      for data, label in dataset.create_tuple_iterator():
          pred = model(data)
          total += len(data)
          test_loss += loss_fn(pred, label).asnumpy()
          correct += (pred.argmax(1) == label).asnumpy().sum()
          all_preds.extend(pred.argmax(1).asnumpy())
          all_labels.extend(label.asnumpy())
      test_loss /= num_batches
      correct /= total
      return test_loss, correct, all_preds, all_labels
  
  def compute_confusion_matrix(y_true, y_pred, num_classes):
      conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int32)
      for true, pred in zip(y_true, y_pred):
          conf_matrix[true, pred] += 1
      return conf_matrix
  
  train_losses = []
  test_losses = []
  test_accuracies = []
  all_preds = []
  all_labels = []
  
  for t in range(epochs):
      train_loss = train_loop(model, train_dataset)
      test_loss, test_accuracy, epoch_preds, epoch_labels = test_loop(model, test_dataset, loss_fn)
      train_losses.append(train_loss)
      test_losses.append(test_loss)
      test_accuracies.append(test_accuracy)
      all_preds.extend(epoch_preds)
      all_labels.extend(epoch_labels)
      print(f"Epoch [{t}], train_loss: {train_loss:.4f}, val_loss: {test_loss:.4f}, val_acc: {test_accuracy:.4f}")
  print("Done!")
  
  # Plotting
  epochs_range = range(1, epochs + 1)
  plt.figure(figsize=(12, 6))
  
  plt.subplot(1, 2, 1)
  plt.plot(epochs_range, train_losses, label='Train Loss')
  plt.plot(epochs_range, test_losses, label='Test Loss')
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.title('Training and Testing Loss')
  plt.legend()
  
  plt.subplot(1, 2, 2)
  plt.plot(epochs_range, [acc * 100 for acc in test_accuracies], label='Test Accuracy')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy (%)')
  plt.title('Test Accuracy')
  plt.legend()
  
  plt.show()
  
  # Confusion Matrix
  cifar10_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
  conf_matrix = compute_confusion_matrix(all_labels[:10000], all_preds[:10000], 10)
  plt.figure(figsize=(10, 8))
  plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)
  plt.title('Confusion Matrix')
  plt.colorbar()
  tick_marks = np.arange(10)
  plt.xticks(tick_marks, cifar10_labels, rotation=45)
  plt.yticks(tick_marks, cifar10_labels)
  plt.xlabel('Predicted Label')
  plt.ylabel('True Label')
  
  # Add text annotations to the confusion matrix
  thresh = conf_matrix.max() / 2
  for i in range(conf_matrix.shape[0]):
      for j in range(conf_matrix.shape[1]):
          plt.text(j, i, format(conf_matrix[i, j], 'd'),
                   horizontalalignment='center',
                   color='white' if conf_matrix[i, j] > thresh else 'black')
  
  plt.show()
  
\end{lstlisting}

\newpage
\section{\centering 心得体会}


% \subsection{关于上课的体会}



% \subsection{关于实验的体会}
本学期的三个实验很好的锻炼了我的代码能力，我通过这些代码的手动实现，更加深刻的理解了课上的专业知识点，
如朴素贝叶斯实现分类和预测，KNN实现分类和预测，神经网络的实现等。同时，通过实验我也学会了如何使用mindspore平台。

在使用华为mindspore平台时，我觉得数据集的加载相比于手动实现的方式更加方便，同时mindspore平台提供了丰富的API接口，
可以方便地实现神经网络的训练和评估。在本次实验中，我实现了一个基于全连接神经网络的分类器，并在CIFAR-10数据集上进行了训绋和评估。
通过本次实验，我对神经网络的训练过程有了更深入的理解，同时也学习了如何使用mindspore平台实现神经网络模型。
希望在以后的实验中，我能够更加熟练地使用mindspore平台，实现更加复杂的神经网络模型。
但是相比于PyTorch, Tensorflow等主流深度学习算法框架，mindspore仍有部分数据集的加载仍然比较复杂，
希望有关开发人员能够进一步优化API接口，提高mindspore的易用性。
% \subsection{总的体会}



\end{document}
